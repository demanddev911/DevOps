{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üåü Google Reviews Collector - BigQuery Integration\n",
        "\n",
        "This notebook fetches Google reviews for all locations from the Map_Location table and stores them in BigQuery.\n",
        "\n",
        "## Features:\n",
        "- üîç Fetches CIDs from Map_Location BigQuery table\n",
        "- üí¨ Scrapes reviews for each location using RapidAPI\n",
        "- üíæ Stores all reviews in a BigQuery Reviews table\n",
        "- üöÄ Batch processing with progress tracking\n",
        "- ‚ú® Automatic deduplication and error handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö° Quick Start Guide\n",
        "\n",
        "**Important: Run cells in order from top to bottom!**\n",
        "\n",
        "1. **Step 1:** Install packages\n",
        "2. **Step 2:** Import libraries  \n",
        "3. **Step 3:** Configure API credentials\n",
        "4. **Step 4:** Initialize BigQuery client\n",
        "5. **Step 5:** Fetch CIDs from Map_Location table\n",
        "\n",
        "If Step 5 shows \"No data\", run Step 5a to diagnose the issue.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Step 1: Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q requests pandas google-cloud-bigquery google-auth db-dtypes\n",
        "print(\"‚úÖ All packages installed successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Step 2: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import http.client\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import datetime, timezone\n",
        "from typing import Optional, Dict, Any, List\n",
        "from google.oauth2 import service_account\n",
        "from google.cloud import bigquery\n",
        "from google.colab import userdata\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Step 5a: Diagnose BigQuery Data (Run this if no data found)\n",
        "\n",
        "‚ö†Ô∏è **Important:** Make sure you've run all previous cells (Steps 1-4) before running this diagnostic.\n",
        "\n",
        "If the previous cell shows no data, run this cell to diagnose the issue:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def diagnose_bigquery_data():\n",
        "    \"\"\"\n",
        "    Comprehensive diagnosis of BigQuery data availability.\n",
        "    \"\"\"\n",
        "    print(\"üîç BIGQUERY DATA DIAGNOSIS\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Check if variables are defined\n",
        "    try:\n",
        "        _ = PROJECT_ID\n",
        "        _ = DATASET_ID\n",
        "        _ = bq_client\n",
        "    except NameError as e:\n",
        "        print(\"‚ùå Configuration variables not found!\")\n",
        "        print(\"\\n‚ö†Ô∏è You need to run the previous cells first:\")\n",
        "        print(\"   1. Step 1: Install packages\")\n",
        "        print(\"   2. Step 2: Import libraries\")\n",
        "        print(\"   3. Step 3: Configure credentials\")\n",
        "        print(\"   4. Step 4: Initialize BigQuery client\")\n",
        "        print(\"\\n   Then come back and run this cell.\")\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        # Check if dataset exists\n",
        "        dataset_id = f\"{PROJECT_ID}.{DATASET_ID}\"\n",
        "        try:\n",
        "            dataset = bq_client.get_dataset(dataset_id)\n",
        "            print(f\"‚úÖ Dataset exists: {dataset_id}\")\n",
        "            print(f\"   Created: {dataset.created}\")\n",
        "            print(f\"   Location: {dataset.location}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Dataset not found: {dataset_id}\")\n",
        "            print(f\"   Error: {e}\")\n",
        "            return\n",
        "        \n",
        "        # List all tables in dataset\n",
        "        print(f\"\\nüìä Tables in {DATASET_ID}:\")\n",
        "        tables = bq_client.list_tables(dataset_id)\n",
        "        table_list = list(tables)\n",
        "        \n",
        "        if not table_list:\n",
        "            print(\"   ‚ö†Ô∏è No tables found in dataset\")\n",
        "            return\n",
        "        \n",
        "        for table in table_list:\n",
        "            full_table = bq_client.get_table(f\"{dataset_id}.{table.table_id}\")\n",
        "            print(f\"   - {table.table_id}: {full_table.num_rows:,} rows\")\n",
        "        \n",
        "        # Check Map_Location table specifically\n",
        "        print(f\"\\nüó∫Ô∏è Checking {SOURCE_TABLE} table:\")\n",
        "        table_id = f\"{PROJECT_ID}.{DATASET_ID}.{SOURCE_TABLE}\"\n",
        "        \n",
        "        try:\n",
        "            table = bq_client.get_table(table_id)\n",
        "            print(f\"   ‚úÖ Table exists\")\n",
        "            print(f\"   üìä Total rows: {table.num_rows:,}\")\n",
        "            \n",
        "            if table.num_rows == 0:\n",
        "                print(\"\\n   ‚ö†Ô∏è TABLE IS EMPTY!\")\n",
        "                print(\"\\n   üí° To populate the table:\")\n",
        "                print(\"      1. Go to your Map_Location_Final.ipynb notebook\")\n",
        "                print(\"      2. Run the data collection cells\")\n",
        "                print(\"      3. Upload locations to BigQuery\")\n",
        "                print(\"      4. Come back here and run this notebook again\")\n",
        "            else:\n",
        "                # Show sample data\n",
        "                sample_query = f\"SELECT * FROM `{table_id}` LIMIT 3\"\n",
        "                sample_df = bq_client.query(sample_query).to_dataframe()\n",
        "                print(\"\\n   üìã Sample data:\")\n",
        "                print(sample_df.to_string())\n",
        "                \n",
        "                # Check for CID column\n",
        "                print(\"\\n   üîë Checking for CID-like columns:\")\n",
        "                cid_cols = [col for col in sample_df.columns if 'id' in col.lower() or 'cid' in col.lower()]\n",
        "                if cid_cols:\n",
        "                    for col in cid_cols:\n",
        "                        non_null = sample_df[col].notna().sum()\n",
        "                        print(f\"      - {col}: {non_null}/{len(sample_df)} non-null values\")\n",
        "                else:\n",
        "                    print(\"      ‚ö†Ô∏è No CID-like columns found!\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Table not found: {SOURCE_TABLE}\")\n",
        "            print(f\"   Error: {e}\")\n",
        "            print(\"\\n   üí° The table needs to be created first.\")\n",
        "            print(\"      Run the Map_Location_Final.ipynb notebook to create it.\")\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during diagnosis: {e}\")\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "\n",
        "# Run diagnosis\n",
        "diagnose_bigquery_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Step 5b: Test with a Single CID (Optional)\n",
        "\n",
        "If you want to test the review scraping before processing all locations, run this cell with a test CID:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TEST WITH A SINGLE CID\n",
        "# Change this to your test CID or leave the default\n",
        "TEST_CID = \"7632417579134624850\"  # Example CID\n",
        "TEST_LOCATION_NAME = \"Test Location\"\n",
        "\n",
        "def test_single_cid(cid: str, location_name: str = \"Test Location\"):\n",
        "    \"\"\"\n",
        "    Test review fetching for a single CID.\n",
        "    \"\"\"\n",
        "    print(f\"üß™ TESTING REVIEW SCRAPING\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"CID: {cid}\")\n",
        "    print(f\"Location: {location_name}\")\n",
        "    print(\"\\nüîÑ Fetching reviews...\\n\")\n",
        "    \n",
        "    try:\n",
        "        # Fetch reviews\n",
        "        reviews = fetch_reviews_for_cid(cid, max_reviews=10)\n",
        "        \n",
        "        if reviews:\n",
        "            print(f\"\\n‚úÖ Successfully fetched {len(reviews)} reviews!\")\n",
        "            \n",
        "            # Convert to DataFrame\n",
        "            reviews_df = reviews_to_dataframe(reviews, cid, location_name)\n",
        "            \n",
        "            print(\"\\nüìä Sample reviews:\")\n",
        "            print(\"=\" * 80)\n",
        "            for idx, row in reviews_df.head(3).iterrows():\n",
        "                print(f\"\\n‚≠ê Rating: {row['rating']}/5\")\n",
        "                print(f\"üë§ User: {row['user_name']}\")\n",
        "                print(f\"üìÖ Date: {row['date']}\")\n",
        "                print(f\"üí¨ Review: {row['snippet'][:150]}...\")\n",
        "                print(\"-\" * 80)\n",
        "            \n",
        "            # Ask if user wants to upload this test data\n",
        "            print(\"\\nüíæ Test data is ready.\")\n",
        "            print(\"   To upload to BigQuery, uncomment the line below and run again:\")\n",
        "            print(\"   # upload_reviews_to_bigquery(reviews_df)\")\n",
        "            \n",
        "            return reviews_df\n",
        "        else:\n",
        "            print(\"‚ùå No reviews found or error occurred.\")\n",
        "            return None\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during test: {e}\")\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "# Run the test (uncomment to use)\n",
        "# test_df = test_single_cid(TEST_CID, TEST_LOCATION_NAME)\n",
        "\n",
        "print(\"‚ÑπÔ∏è To test review scraping:\")\n",
        "print(\"   1. Set TEST_CID to a valid Google Maps CID\")\n",
        "print(\"   2. Uncomment the last line: test_df = test_single_cid(...)\")\n",
        "print(\"   3. Run this cell\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîë Step 3: Configure API Credentials\n",
        "\n",
        "### Option A: Using Colab Secrets (Recommended)\n",
        "1. Click on the üîë key icon in the left sidebar\n",
        "2. Add a secret named `RAPIDAPI_KEY` with your API key\n",
        "3. Add a secret named `BIGQUERY_KEY_JSON` with your service account JSON\n",
        "\n",
        "### Option B: Manual Configuration\n",
        "Uncomment and fill in the credentials below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# RapidAPI Configuration\n",
        "RAPIDAPI_HOST = \"google-search-master-mega.p.rapidapi.com\"\n",
        "\n",
        "# Try to get credentials from Colab secrets first\n",
        "try:\n",
        "    RAPIDAPI_KEY = userdata.get('RAPIDAPI_KEY')\n",
        "    print(\"‚úÖ RapidAPI key loaded from Colab secrets\")\n",
        "except:\n",
        "    # Manual configuration - uncomment and fill in\n",
        "    RAPIDAPI_KEY = \"ac0025f410mshd0c260cb60f3db6p18c4b0jsnc9b7413cd574\"  # Your API key\n",
        "    print(\"‚ö†Ô∏è RapidAPI key loaded from manual configuration\")\n",
        "\n",
        "# Load BigQuery credentials from secrets\n",
        "try:\n",
        "    BIGQUERY_CREDENTIALS_STR = userdata.get('BIGQUERY_KEY_JSON')\n",
        "    BIGQUERY_CREDENTIALS = json.loads(BIGQUERY_CREDENTIALS_STR)\n",
        "    print(\"‚úÖ BigQuery credentials loaded from Colab secrets\")\n",
        "    PROJECT_ID = BIGQUERY_CREDENTIALS.get('project_id', 'shopper-reviews-477306')\n",
        "except:\n",
        "    # Fallback to manual configuration\n",
        "    print(\"‚ö†Ô∏è BigQuery credentials loaded from manual configuration\")\n",
        "    PROJECT_ID = \"shopper-reviews-477306\"\n",
        "    BIGQUERY_CREDENTIALS = {\n",
        "        \"type\": \"service_account\",\n",
        "        \"project_id\": \"shopper-reviews-477306\",\n",
        "        \"private_key_id\": \"679b00310997262ff77901f080075b509eb9c770\",\n",
        "        \"private_key\": \"-----BEGIN PRIVATE KEY-----\\\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCPrVXmepJWx8A8\\\\nXLqDARbLqqmgPwQ4NEmCCOmAZ019aFToc0Yho0/hDyMhRhsW6z/5h8YVEbheb2oR\\\\nmFK6/v3UEN1Mf6oJWag9pBngM6IO96QAzozjXjCmIVYJku1HWi+7b4mX7La8p77N\\\\n5fJdOh30ceC6cJSDA51r2xGJDmchRPNhRR8CS9u3xAeZZeB/pgShwJcLM4WY4L3P\\\\niwc7qkQb91NPbB2/p3hL/JJAtCvVKf61xlWGOKEGW3pIwBUUcF2/OJ3FTuWrY7P8\\\\n1c/Kz9LUYOZpztK9zjFCNcnCQvvVAow9bqg3fw6xqE172dQT1FG6AieFSCyUib5B\\\\nXxwNu0phAgMBAAECggEAET1ThPqIxqA54RmgnjQqP7k0Q0XBxDCvRUq7zIFuBdyC\\\\nm6Wr8OtUnAT3Snh2qv2tSSFRKO6zDaRsDhJrPYQigX3zNR5Nu8jQlseIUfjqusWy\\\\nHbqq+GPb4y3gJ06Zk/8uolyUHkZJTZe0cvuNZOxNSIBwM6QV3dE4OVx+3SV88GZ/\\\\nOkAMCUpPRLJux6vJo+l0Qcfe074qjRYPv3XUaGXyHXeOZXmze/lLF6wsEzZmP1A+\\\\nE9xZmP4ucM3ybrYi3ipRu6YwuR2mRASLy8VFMtcYCvNZGv6ODkjF2xmpucHwX78S\\\\nzO3mGFES3Hnknjzoif5sJuBewNSztXJcQqKgtSpDhQKBgQDCS6bYj1VR691J5wxA\\\\n5/fl2MwY4ALIKqW4RtJyNRBZ7+WDAVkq99R6lz+AmQsb6QyiZ/yTZHSUI61Bjn0p\\\\nd2MD/fpQle7ZOMyR1gKZk5fE5lvmfA5sK+Aax3dRI7xjPBXJYI4hiCMAxgYdhgtI\\\\nG1C/Nf6O2HoE/W2qLEnLZadpowKBgQC9Tl+/9Eq9Q/DI74CG78U0+s2aRq19vsXZ\\\\n+wCIUm54TcN9xw4nPKYbT24nTVwTrOu2bxEgDVmuAqtWlKGad16LqZFTZ2aUaEFC\\\\ni1HL8UKSy5XmNcum8mrKL5+MvwExcQUSmalE3PEQDRjV65QNld0EbQ6JNz74025z\\\\nm+3ISpIEKwKBgADf5E1fP8wRmrplbtmv8Z64PhryjzCleH9+2h2nfX5aJRdU3zjh\\\\nSrSOj7uddL5YazUj8LAdKKUuD+6WnJueLPTspL7OHfgeWFVjuDlGv80kGE/OSSZV\\\\ngDm+ohvcZFGyCIsSgzFFcprjSU3Ct7RIYzGpJY8xDEOPfHninyZqO7mvAoGAIsog\\\\ndppikd3Ghmbda+7sgwwEdPHAOHeyzJiARI1BmAJShu7p/vP6YtJ6H+broQIKX4CR\\\\n2R4a+QusiUDPYh/F1EzZVEaQZ32xYJVR9vTjky6u4ZvJTWkHjxipbag8g+WNVRnA\\\\nLdOcyaJeihG9J7H+6C1Smoz4manhhoWFcWWi5/kCgYEAssgWnlZCygCjEQ/XDVtZ\\\\nC8/uelJnMHO93U4yF6Xk61gazKYpXpKjNkD3xfxAyQ3zkBkWo7CXg1env8pT9ld1\\\\nraWCeCmH/w8i0ww3Cmplks5mXIYPrPPuUCEW5D6B8hIyNC1VIoaOlva8+FgJYPIv\\\\nC5AqN3hBRDOUbophIQmAe5I=\\\\n-----END PRIVATE KEY-----\\\\n\",\n",
        "        \"client_email\": \"demand@shopper-reviews-477306.iam.gserviceaccount.com\",\n",
        "        \"client_id\": \"100956109416744224832\",\n",
        "        \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
        "        \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
        "        \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
        "        \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/demand%40shopper-reviews-477306.iam.gserviceaccount.com\",\n",
        "        \"universe_domain\": \"googleapis.com\"\n",
        "    }\n",
        "\n",
        "# BigQuery Configuration\n",
        "DATASET_ID = \"shopper_reviews_db\"\n",
        "SOURCE_TABLE = \"Map_location\"  # Table with CIDs\n",
        "REVIEWS_TABLE = \"Reviews\"       # New table for reviews\n",
        "\n",
        "# Review scraping settings\n",
        "LANG = \"en\"\n",
        "COUNTRY = \"us\"\n",
        "SORT_BY = \"newest\"\n",
        "MAX_REVIEWS_PER_LOCATION = 100  # Set max reviews per location\n",
        "\n",
        "print(\"\\n‚úÖ All credentials configured successfully!\")\n",
        "print(f\"üìä Source Table: {PROJECT_ID}.{DATASET_ID}.{SOURCE_TABLE}\")\n",
        "print(f\"üìä Reviews Table: {PROJECT_ID}.{DATASET_ID}.{REVIEWS_TABLE}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîå Step 4: Initialize BigQuery Client"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_bigquery_client() -> Optional[bigquery.Client]:\n",
        "    \"\"\"\n",
        "    Initialize and return BigQuery client.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        credentials = service_account.Credentials.from_service_account_info(\n",
        "            BIGQUERY_CREDENTIALS,\n",
        "            scopes=[\"https://www.googleapis.com/auth/bigquery\"]\n",
        "        )\n",
        "        client = bigquery.Client(\n",
        "            credentials=credentials,\n",
        "            project=PROJECT_ID\n",
        "        )\n",
        "        logger.info(\"‚úÖ BigQuery client initialized\")\n",
        "        return client\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Failed to initialize BigQuery client: {e}\")\n",
        "        return None\n",
        "\n",
        "# Test connection\n",
        "bq_client = get_bigquery_client()\n",
        "if bq_client:\n",
        "    print(\"‚úÖ BigQuery connection established!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìç Step 5: Fetch CIDs from Map_Location Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def check_table_structure():\n",
        "    \"\"\"\n",
        "    Check the Map_Location table structure and identify CID column.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        table_id = f\"{PROJECT_ID}.{DATASET_ID}.{SOURCE_TABLE}\"\n",
        "        table = bq_client.get_table(table_id)\n",
        "        \n",
        "        print(\"\\nüìã TABLE SCHEMA:\")\n",
        "        print(\"=\" * 80)\n",
        "        for field in table.schema:\n",
        "            print(f\"  - {field.name}: {field.field_type}\")\n",
        "        \n",
        "        print(f\"\\nüìä Total rows in table: {table.num_rows:,}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error checking table: {e}\")\n",
        "        return False\n",
        "\n",
        "def get_location_cids() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetch all CIDs and location info from Map_Location table.\n",
        "    Automatically detects the correct CID column name.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # First, get a sample to check column names\n",
        "        sample_query = f\"\"\"\n",
        "        SELECT *\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.{SOURCE_TABLE}`\n",
        "        LIMIT 5\n",
        "        \"\"\"\n",
        "        \n",
        "        logger.info(\"üîç Checking Map_Location table structure...\")\n",
        "        sample_df = bq_client.query(sample_query).to_dataframe()\n",
        "        \n",
        "        if sample_df.empty:\n",
        "            logger.warning(\"‚ö†Ô∏è Map_Location table is empty!\")\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        # Find the CID column\n",
        "        cid_column = None\n",
        "        for possible_name in ['cid', 'place_id', 'placeId', 'id']:\n",
        "            if possible_name in sample_df.columns:\n",
        "                cid_column = possible_name\n",
        "                logger.info(f\"‚úÖ Found CID column: '{cid_column}'\")\n",
        "                break\n",
        "        \n",
        "        if not cid_column:\n",
        "            logger.error(\"‚ùå Could not find CID column in table!\")\n",
        "            print(\"\\nüìã Available columns:\", list(sample_df.columns))\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        # Build query with the correct column name\n",
        "        query = f\"\"\"\n",
        "        SELECT \n",
        "            {cid_column} as cid,\n",
        "            title,\n",
        "            address,\n",
        "            rating,\n",
        "            reviews_count\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.{SOURCE_TABLE}`\n",
        "        WHERE {cid_column} IS NOT NULL\n",
        "        ORDER BY title\n",
        "        \"\"\"\n",
        "        \n",
        "        logger.info(\"üîç Fetching all locations from Map_Location table...\")\n",
        "        df = bq_client.query(query).to_dataframe()\n",
        "        logger.info(f\"‚úÖ Found {len(df)} locations with CIDs\")\n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error fetching CIDs: {e}\")\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Check table structure first\n",
        "print(\"üîç Checking Map_Location table...\")\n",
        "check_table_structure()\n",
        "\n",
        "# Fetch locations\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "locations_df = get_location_cids()\n",
        "\n",
        "if not locations_df.empty:\n",
        "    print(f\"\\n‚úÖ Total locations to process: {len(locations_df)}\")\n",
        "    print(\"\\nüìç First 5 locations:\")\n",
        "    print(locations_df.head())\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No locations found in Map_Location table!\")\n",
        "    print(\"\\n‚ÑπÔ∏è Please make sure:\")\n",
        "    print(\"  1. The Map_Location table has data\")\n",
        "    print(\"  2. The table has a CID/place_id column\")\n",
        "    print(\"  3. You have the correct project/dataset/table names\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí¨ Step 6: Define Review Fetching Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def fetch_reviews_for_cid(cid: str, max_reviews: int = 100) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Fetch reviews for a specific CID from RapidAPI.\n",
        "    \n",
        "    Args:\n",
        "        cid: The Google Maps CID\n",
        "        max_reviews: Maximum number of reviews to fetch\n",
        "    \n",
        "    Returns:\n",
        "        List of review dictionaries\n",
        "    \"\"\"\n",
        "    conn = http.client.HTTPSConnection(RAPIDAPI_HOST)\n",
        "    headers = {\n",
        "        \"x-rapidapi-key\": RAPIDAPI_KEY,\n",
        "        \"x-rapidapi-host\": RAPIDAPI_HOST\n",
        "    }\n",
        "    \n",
        "    all_reviews = []\n",
        "    page = 1\n",
        "    \n",
        "    try:\n",
        "        # First request to get total reviews count\n",
        "        conn.request(\n",
        "            \"GET\",\n",
        "            f\"/reviews?cid={cid}&sortBy={SORT_BY}&gl={COUNTRY}&hl={LANG}&page=1\",\n",
        "            headers=headers\n",
        "        )\n",
        "        res = conn.getresponse()\n",
        "        data = res.read()\n",
        "        \n",
        "        if res.status != 200:\n",
        "            logger.warning(f\"‚ö†Ô∏è Failed to fetch reviews for CID {cid}: {res.status}\")\n",
        "            return []\n",
        "        \n",
        "        json_data = json.loads(data.decode(\"utf-8\"))\n",
        "        place_info = json_data.get(\"placeInfo\", {})\n",
        "        total_reviews = place_info.get(\"reviewsCount\") or json_data.get(\"reviewsCount\", 0)\n",
        "        \n",
        "        logger.info(f\"üìç {place_info.get('title', 'Unknown')} - Total reviews: {total_reviews}\")\n",
        "        \n",
        "        # Determine how many to scrape\n",
        "        to_scrape = min(max_reviews, total_reviews) if total_reviews else max_reviews\n",
        "        \n",
        "        # Fetch reviews page by page\n",
        "        while len(all_reviews) < to_scrape:\n",
        "            if page > 1:  # Already have page 1 data\n",
        "                conn.request(\n",
        "                    \"GET\",\n",
        "                    f\"/reviews?cid={cid}&sortBy={SORT_BY}&gl={COUNTRY}&hl={LANG}&page={page}\",\n",
        "                    headers=headers\n",
        "                )\n",
        "                res = conn.getresponse()\n",
        "                data = res.read()\n",
        "                \n",
        "                if res.status != 200:\n",
        "                    logger.warning(f\"‚ö†Ô∏è Error on page {page}: {res.status}\")\n",
        "                    break\n",
        "                \n",
        "                json_data = json.loads(data.decode(\"utf-8\"))\n",
        "            \n",
        "            reviews = json_data.get(\"reviews\", [])\n",
        "            if not reviews:\n",
        "                logger.info(f\"‚úÖ No more reviews found at page {page}\")\n",
        "                break\n",
        "            \n",
        "            all_reviews.extend(reviews)\n",
        "            logger.info(f\"  üìÑ Page {page}: Collected {len(all_reviews)}/{to_scrape} reviews\")\n",
        "            \n",
        "            if len(all_reviews) >= to_scrape:\n",
        "                break\n",
        "            \n",
        "            page += 1\n",
        "            time.sleep(1)  # Rate limiting\n",
        "        \n",
        "        # Trim to max_reviews\n",
        "        all_reviews = all_reviews[:max_reviews]\n",
        "        logger.info(f\"‚úÖ Scraped {len(all_reviews)} reviews for CID {cid}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error fetching reviews for CID {cid}: {e}\")\n",
        "    finally:\n",
        "        conn.close()\n",
        "    \n",
        "    return all_reviews\n",
        "\n",
        "print(\"‚úÖ Review fetching functions defined!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Step 7: Convert Reviews to DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def reviews_to_dataframe(reviews: List[Dict[str, Any]], cid: str, location_title: str = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Convert raw review data to a structured DataFrame.\n",
        "    \n",
        "    Args:\n",
        "        reviews: List of review dictionaries from API\n",
        "        cid: The CID of the location\n",
        "        location_title: Optional title of the location\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with structured review data\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame([\n",
        "        {\n",
        "            \"cid\": cid,\n",
        "            \"location_title\": location_title,\n",
        "            \"review_id\": r.get(\"id\"),\n",
        "            \"rating\": r.get(\"rating\"),\n",
        "            \"snippet\": r.get(\"snippet\"),\n",
        "            \"likes\": r.get(\"likes\"),\n",
        "            \"date\": r.get(\"date\"),\n",
        "            \"iso_date\": r.get(\"isoDate\"),\n",
        "            \"user_name\": r.get(\"user\", {}).get(\"name\"),\n",
        "            \"user_profile\": r.get(\"user\", {}).get(\"link\"),\n",
        "            \"user_thumbnail\": r.get(\"user\", {}).get(\"thumbnail\"),\n",
        "            \"user_reviews_count\": r.get(\"user\", {}).get(\"reviews\"),\n",
        "            \"user_photos_count\": r.get(\"user\", {}).get(\"photos\"),\n",
        "            \"scraped_at\": datetime.now(timezone.utc).isoformat()\n",
        "        }\n",
        "        for r in reviews\n",
        "    ])\n",
        "    \n",
        "    return df\n",
        "\n",
        "print(\"‚úÖ DataFrame conversion function defined!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Step 8: Create Reviews Table in BigQuery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_reviews_table() -> bool:\n",
        "    \"\"\"\n",
        "    Create the Reviews table in BigQuery if it doesn't exist.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        table_id = f\"{PROJECT_ID}.{DATASET_ID}.{REVIEWS_TABLE}\"\n",
        "        \n",
        "        # Check if table exists\n",
        "        try:\n",
        "            bq_client.get_table(table_id)\n",
        "            logger.info(f\"‚úÖ Table {table_id} already exists\")\n",
        "            return True\n",
        "        except:\n",
        "            logger.info(f\"üìù Creating table {table_id}...\")\n",
        "        \n",
        "        # Define schema\n",
        "        schema = [\n",
        "            bigquery.SchemaField(\"cid\", \"STRING\", mode=\"REQUIRED\"),\n",
        "            bigquery.SchemaField(\"location_title\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"review_id\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"rating\", \"INTEGER\"),\n",
        "            bigquery.SchemaField(\"snippet\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"likes\", \"INTEGER\"),\n",
        "            bigquery.SchemaField(\"date\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"iso_date\", \"TIMESTAMP\"),\n",
        "            bigquery.SchemaField(\"user_name\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"user_profile\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"user_thumbnail\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"user_reviews_count\", \"INTEGER\"),\n",
        "            bigquery.SchemaField(\"user_photos_count\", \"INTEGER\"),\n",
        "            bigquery.SchemaField(\"scraped_at\", \"TIMESTAMP\")\n",
        "        ]\n",
        "        \n",
        "        table = bigquery.Table(table_id, schema=schema)\n",
        "        table = bq_client.create_table(table)\n",
        "        logger.info(f\"‚úÖ Created table {table_id}\")\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error creating table: {e}\")\n",
        "        return False\n",
        "\n",
        "# Create table\n",
        "if create_reviews_table():\n",
        "    print(\"‚úÖ Reviews table is ready!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì§ Step 9: Upload Reviews to BigQuery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def upload_reviews_to_bigquery(df: pd.DataFrame) -> bool:\n",
        "    \"\"\"\n",
        "    Upload reviews DataFrame to BigQuery.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame containing review data\n",
        "    \n",
        "    Returns:\n",
        "        True if successful, False otherwise\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        logger.warning(\"‚ö†Ô∏è No reviews to upload\")\n",
        "        return False\n",
        "    \n",
        "    try:\n",
        "        table_id = f\"{PROJECT_ID}.{DATASET_ID}.{REVIEWS_TABLE}\"\n",
        "        \n",
        "        # Configure job\n",
        "        job_config = bigquery.LoadJobConfig(\n",
        "            write_disposition=\"WRITE_APPEND\",  # Append to existing table\n",
        "            schema_update_options=[bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION]\n",
        "        )\n",
        "        \n",
        "        # Upload\n",
        "        job = bq_client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
        "        job.result()  # Wait for completion\n",
        "        \n",
        "        logger.info(f\"‚úÖ Uploaded {len(df)} reviews to BigQuery\")\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error uploading to BigQuery: {e}\")\n",
        "        return False\n",
        "\n",
        "print(\"‚úÖ Upload function defined!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Step 10: Main Processing Loop\n",
        "\n",
        "This cell processes all locations and fetches their reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def process_all_locations(locations_df: pd.DataFrame, max_reviews_per_location: int = 100, batch_size: int = 10):\n",
        "    \"\"\"\n",
        "    Process all locations and fetch their reviews.\n",
        "    \n",
        "    Args:\n",
        "        locations_df: DataFrame with location data\n",
        "        max_reviews_per_location: Max reviews to fetch per location\n",
        "        batch_size: Number of locations to process before uploading to BigQuery\n",
        "    \"\"\"\n",
        "    total_locations = len(locations_df)\n",
        "    total_reviews_collected = 0\n",
        "    failed_locations = []\n",
        "    \n",
        "    print(f\"\\nüöÄ Starting to process {total_locations} locations...\")\n",
        "    print(f\"üìä Will fetch up to {max_reviews_per_location} reviews per location\\n\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    batch_reviews = []\n",
        "    \n",
        "    for idx, row in locations_df.iterrows():\n",
        "        cid = row['cid']\n",
        "        title = row.get('title', 'Unknown')\n",
        "        \n",
        "        print(f\"\\n[{idx + 1}/{total_locations}] Processing: {title}\")\n",
        "        print(f\"  CID: {cid}\")\n",
        "        \n",
        "        try:\n",
        "            # Fetch reviews\n",
        "            reviews = fetch_reviews_for_cid(cid, max_reviews_per_location)\n",
        "            \n",
        "            if reviews:\n",
        "                # Convert to DataFrame\n",
        "                reviews_df = reviews_to_dataframe(reviews, cid, title)\n",
        "                batch_reviews.append(reviews_df)\n",
        "                total_reviews_collected += len(reviews_df)\n",
        "                print(f\"  ‚úÖ Collected {len(reviews_df)} reviews\")\n",
        "            else:\n",
        "                print(f\"  ‚ö†Ô∏è No reviews found\")\n",
        "            \n",
        "            # Upload batch if we've reached batch_size\n",
        "            if len(batch_reviews) >= batch_size:\n",
        "                combined_df = pd.concat(batch_reviews, ignore_index=True)\n",
        "                print(f\"\\nüì§ Uploading batch of {len(combined_df)} reviews to BigQuery...\")\n",
        "                if upload_reviews_to_bigquery(combined_df):\n",
        "                    print(f\"‚úÖ Batch uploaded successfully!\")\n",
        "                    batch_reviews = []  # Clear batch\n",
        "                else:\n",
        "                    print(f\"‚ùå Failed to upload batch\")\n",
        "            \n",
        "            # Rate limiting between locations\n",
        "            time.sleep(2)\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Error processing {title} (CID: {cid}): {e}\")\n",
        "            failed_locations.append({'cid': cid, 'title': title, 'error': str(e)})\n",
        "    \n",
        "    # Upload remaining reviews\n",
        "    if batch_reviews:\n",
        "        combined_df = pd.concat(batch_reviews, ignore_index=True)\n",
        "        print(f\"\\nüì§ Uploading final batch of {len(combined_df)} reviews...\")\n",
        "        upload_reviews_to_bigquery(combined_df)\n",
        "    \n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"\\nüìä PROCESSING SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"‚úÖ Total locations processed: {total_locations}\")\n",
        "    print(f\"üí¨ Total reviews collected: {total_reviews_collected}\")\n",
        "    print(f\"‚ùå Failed locations: {len(failed_locations)}\")\n",
        "    \n",
        "    if failed_locations:\n",
        "        print(\"\\n‚ö†Ô∏è Failed locations:\")\n",
        "        for loc in failed_locations:\n",
        "            print(f\"  - {loc['title']} (CID: {loc['cid']}): {loc['error']}\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Processing complete!\")\n",
        "    return total_reviews_collected, failed_locations\n",
        "\n",
        "# Run the main process\n",
        "if not locations_df.empty:\n",
        "    total_reviews, failed = process_all_locations(\n",
        "        locations_df,\n",
        "        max_reviews_per_location=MAX_REVIEWS_PER_LOCATION,\n",
        "        batch_size=10\n",
        "    )\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No locations found to process\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 11: Verify Results in BigQuery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_review_stats():\n",
        "    \"\"\"\n",
        "    Get statistics about the collected reviews.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        query = f\"\"\"\n",
        "        SELECT \n",
        "            COUNT(*) as total_reviews,\n",
        "            COUNT(DISTINCT cid) as unique_locations,\n",
        "            AVG(rating) as avg_rating,\n",
        "            MIN(iso_date) as earliest_review,\n",
        "            MAX(iso_date) as latest_review\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.{REVIEWS_TABLE}`\n",
        "        \"\"\"\n",
        "        \n",
        "        df = bq_client.query(query).to_dataframe()\n",
        "        print(\"\\nüìä REVIEWS DATABASE STATISTICS\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"Total Reviews: {df['total_reviews'].iloc[0]:,}\")\n",
        "        print(f\"Unique Locations: {df['unique_locations'].iloc[0]}\")\n",
        "        print(f\"Average Rating: {df['avg_rating'].iloc[0]:.2f}\")\n",
        "        print(f\"Date Range: {df['earliest_review'].iloc[0]} to {df['latest_review'].iloc[0]}\")\n",
        "        \n",
        "        # Top locations by review count\n",
        "        query2 = f\"\"\"\n",
        "        SELECT \n",
        "            location_title,\n",
        "            COUNT(*) as review_count,\n",
        "            AVG(rating) as avg_rating\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.{REVIEWS_TABLE}`\n",
        "        GROUP BY location_title\n",
        "        ORDER BY review_count DESC\n",
        "        LIMIT 10\n",
        "        \"\"\"\n",
        "        \n",
        "        df2 = bq_client.query(query2).to_dataframe()\n",
        "        print(\"\\nüèÜ TOP 10 LOCATIONS BY REVIEW COUNT\")\n",
        "        print(\"=\" * 80)\n",
        "        print(df2.to_string(index=False))\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error getting stats: {e}\")\n",
        "\n",
        "# Get stats\n",
        "get_review_stats()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Step 12 (Optional): Export Reviews to CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def export_reviews_to_csv(filename: str = \"all_reviews.csv\"):\n",
        "    \"\"\"\n",
        "    Export all reviews from BigQuery to a CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        query = f\"\"\"\n",
        "        SELECT *\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.{REVIEWS_TABLE}`\n",
        "        ORDER BY iso_date DESC\n",
        "        \"\"\"\n",
        "        \n",
        "        print(f\"üì• Downloading reviews from BigQuery...\")\n",
        "        df = bq_client.query(query).to_dataframe()\n",
        "        \n",
        "        print(f\"üíæ Saving to {filename}...\")\n",
        "        df.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
        "        print(f\"‚úÖ Saved {len(df)} reviews to {filename}\")\n",
        "        \n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error exporting to CSV: {e}\")\n",
        "        return None\n",
        "\n",
        "# Uncomment to export\n",
        "# reviews_csv = export_reviews_to_csv(\"google_reviews_all.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Summary\n",
        "\n",
        "This notebook successfully:\n",
        "1. ‚úÖ Fetched all CIDs from the Map_Location table\n",
        "2. ‚úÖ Scraped reviews for each location using RapidAPI\n",
        "3. ‚úÖ Stored all reviews in the BigQuery Reviews table\n",
        "4. ‚úÖ Provided statistics and verification\n",
        "\n",
        "### Next Steps:\n",
        "- Analyze review sentiment\n",
        "- Identify trends and patterns\n",
        "- Generate insights for each location\n",
        "- Schedule periodic updates to fetch new reviews"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}