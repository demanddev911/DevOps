# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19gjS7kIXMsiaCGPIPuSBHS0fGxN-zlUC
"""

# --- 1. INSTALL LIBRARIES ---
!pip install pandas-gbq google-auth

# --- 2. IMPORTS ---
import requests
import pandas as pd
from typing import Optional, Dict, Any, List
from google.colab import userdata
import json
from google.oauth2 import service_account
from google.auth.exceptions import DefaultCredentialsError
from IPython.display import display # For nice table output

# --- 3. IN-MEMORY CACHE ---
API_CACHE: Dict[str, Any] = {}

# --- 4. CONFIGURATION ---
# These names MUST match your Colab Secrets
BIGQUERY_KEY_SECRET_NAME = 'BIGQUERY_KEY_JSON'
RAPIDAPI_KEY_SECRET_NAME = 'RAPIDAPI_KEY'

# Your GCP project and dataset
PROJECT_ID = 'shopper-reviews-477306'
DATASET_ID = 'place_data'
TABLE_ID = 'Map_location'

print("âœ… Setup Complete for Google Colab. Proceed to Part 1.")

# --- FUNCTIONS FOR PART 1 ---

def search_by_place_name(place_name: str) -> Optional[Dict[str, Any]]:
    """
    Fetches data for a *single* query from the RapidAPI
    using the key from Colab Secrets.
    """

    if place_name in API_CACHE:
        print(f"\n--- Loading '{place_name}' from CACHE ---")
        return API_CACHE[place_name]

    print(f"\n--- Calling API for '{place_name}' ---")

    API_KEY = userdata.get(RAPIDAPI_KEY_SECRET_NAME)
    API_HOST = "google-search-master-mega.p.rapidapi.com"

    if not API_KEY:
        print(f"Error: '{RAPIDAPI_KEY_SECRET_NAME}' secret not found or is empty.")
        return None

    url = f"https://{API_HOST}/maps"
    querystring = {"q": place_name, "hl": "en", "page": "1"}
    headers = {"x-rapidapi-key": API_KEY, "x-rapidapi-host": API_HOST}

    try:
        response = requests.get(url, headers=headers, params=querystring, timeout=10)

        if response.status_code == 200:
            data = response.json()
            API_CACHE[place_name] = data
            return data
        else:
            print(f"Error: API returned status code {response.status_code}")
            print(f"Response: {response.text}")
            return None

    except requests.exceptions.RequestException as e:
        print(f"An error occurred with the request: {e}")
        return None

def run_data_collection_loop() -> Optional[pd.DataFrame]:
    """
    Loops to get user queries and collects them in a DataFrame.
    """
    all_dataframes_list: List[pd.DataFrame] = []

    print("\nWelcome to the Place Searcher. (Type 'exit' to quit)")

    while True:
        query = input("\nEnter the place name to search for: ")

        if query.lower() == 'exit':
            print("Exiting data collection...")
            break

        if query:
            results_data = search_by_place_name(query)

            if results_data and 'places' in results_data and results_data['places']:
                try:
                    df = pd.json_normalize(results_data['places'])
                    df['search_query'] = query
                    all_dataframes_list.append(df)
                    print(f"âœ… Collected {len(df)} places for '{query}'.")
                except Exception as e:
                    print(f"Error processing data for '{query}': {e}")
            else:
                print(f"No 'places' found for '{query}'.")
        else:
            print("No search query entered. Please try again.")

    if not all_dataframes_list:
        print("No data was collected.")
        return None

    return pd.concat(all_dataframes_list, ignore_index=True)


# --- EXECUTE PART 1 ---
print("--- ðŸš€ PART 1: Get Data ---")

# This variable will be saved in your Colab session
collected_data_df = run_data_collection_loop()

if collected_data_df is not None and not collected_data_df.empty:
    print(f"\n--- âœ… Data Collection Complete ---")
    print(f"Total places collected: {len(collected_data_df)}")
    print("Displaying first 5 rows for validation:")
    display(collected_data_df.head()) # This is your validation step
else:
    print("\nNo data was collected.")
