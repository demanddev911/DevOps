{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üó∫Ô∏è Map Location Data Collector for Google Colab\n",
    "\n",
    "This notebook collects location data from RapidAPI and uploads to BigQuery.\n",
    "\n",
    "## üìã Setup Instructions:\n",
    "\n",
    "1. **Set up Colab Secrets** (üîë icon in left sidebar):\n",
    "   - `RAPIDAPI_KEY` - Your RapidAPI key\n",
    "   - `BIGQUERY_KEY_JSON` - Your BigQuery service account JSON (as a string)\n",
    "\n",
    "2. **Run cells in order**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas-gbq google-auth google-cloud-bigquery db-dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Optional, Dict, Any, List\n",
    "from google.colab import userdata\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 3: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Update these if needed\n",
    "PROJECT_ID = 'shopper-reviews-477306'\n",
    "DATASET_ID = 'place_data'\n",
    "TABLE_ID = 'Map_location'\n",
    "\n",
    "# Colab Secret names\n",
    "RAPIDAPI_KEY_SECRET = 'RAPIDAPI_KEY'\n",
    "BIGQUERY_KEY_SECRET = 'BIGQUERY_KEY_JSON'\n",
    "\n",
    "# In-memory cache for API calls\n",
    "API_CACHE: Dict[str, Any] = {}\n",
    "\n",
    "print(\"‚úÖ Configuration loaded!\")\n",
    "print(f\"   Project: {PROJECT_ID}\")\n",
    "print(f\"   Dataset: {DATASET_ID}\")\n",
    "print(f\"   Table: {TABLE_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 4: Define Data Collection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_place_name(place_name: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetches data for a single place from RapidAPI.\n",
    "    Uses Colab Secrets for API key.\n",
    "    \"\"\"\n",
    "    # Check cache first\n",
    "    if place_name in API_CACHE:\n",
    "        print(f\"üì¶ Loading '{place_name}' from cache\")\n",
    "        return API_CACHE[place_name]\n",
    "\n",
    "    print(f\"üîç Searching for '{place_name}'...\")\n",
    "\n",
    "    # Get API key from Colab Secrets\n",
    "    try:\n",
    "        API_KEY = userdata.get(RAPIDAPI_KEY_SECRET)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: Could not get '{RAPIDAPI_KEY_SECRET}' from Colab Secrets\")\n",
    "        print(f\"   Make sure to add it in the Secrets panel (üîë icon)\")\n",
    "        return None\n",
    "\n",
    "    API_HOST = \"google-search-master-mega.p.rapidapi.com\"\n",
    "    url = f\"https://{API_HOST}/maps\"\n",
    "    \n",
    "    querystring = {\"q\": place_name, \"hl\": \"en\", \"page\": \"1\"}\n",
    "    headers = {\n",
    "        \"x-rapidapi-key\": API_KEY,\n",
    "        \"x-rapidapi-host\": API_HOST\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=querystring, timeout=10)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            API_CACHE[place_name] = data  # Cache the result\n",
    "            print(f\"‚úÖ Found data for '{place_name}'\")\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"‚ùå API Error: Status code {response.status_code}\")\n",
    "            print(f\"   Response: {response.text[:200]}\")\n",
    "            return None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Request error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def collect_places_for_query(query: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Collects and normalizes place data for a single query.\n",
    "    \"\"\"\n",
    "    results_data = search_by_place_name(query)\n",
    "\n",
    "    if results_data and 'places' in results_data and results_data['places']:\n",
    "        try:\n",
    "            df = pd.json_normalize(results_data['places'])\n",
    "            df['search_query'] = query\n",
    "            print(f\"‚úÖ Collected {len(df)} places for '{query}'\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing data: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No places found for '{query}'\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def run_data_collection_loop() -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Interactive loop to collect user queries.\n",
    "    Type 'exit' to stop collecting.\n",
    "    \"\"\"\n",
    "    all_dataframes_list: List[pd.DataFrame] = []\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üó∫Ô∏è  PLACE SEARCHER\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Type place names to search. Type 'exit' when done.\\n\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nüîç Enter place name: \").strip()\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n‚èπÔ∏è Stopping data collection...\")\n",
    "            break\n",
    "\n",
    "        if query.lower() == 'exit':\n",
    "            print(\"‚èπÔ∏è Exiting data collection...\")\n",
    "            break\n",
    "\n",
    "        if query:\n",
    "            df = collect_places_for_query(query)\n",
    "            if df is not None:\n",
    "                all_dataframes_list.append(df)\n",
    "                print(f\"üìä Total queries collected: {len(all_dataframes_list)}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Please enter a place name\")\n",
    "\n",
    "    if not all_dataframes_list:\n",
    "        print(\"\\n‚ö†Ô∏è No data was collected\")\n",
    "        return None\n",
    "\n",
    "    return pd.concat(all_dataframes_list, ignore_index=True)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Data collection functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 5: COLLECT DATA (Interactive)\n",
    "\n",
    "Run this cell and enter place names one by one. Type `exit` when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data interactively\n",
    "collected_data_df = run_data_collection_loop()\n",
    "\n",
    "# Display results\n",
    "if collected_data_df is not None and not collected_data_df.empty:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ DATA COLLECTION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìä Total places collected: {len(collected_data_df)}\")\n",
    "    print(f\"üìã Total columns: {len(collected_data_df.columns)}\")\n",
    "    print(\"\\nüîç Preview (first 5 rows):\")\n",
    "    display(collected_data_df.head())\n",
    "    print(\"\\n‚úÖ Data is ready for upload!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No data to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Step 5b: ALTERNATIVE - Batch Collection (Optional)\n",
    "\n",
    "Use this instead of Step 5 if you want to process multiple places at once without interactive input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Batch mode - Define your places here\n",
    "places_to_search = [\n",
    "    \"Pizza New York\",\n",
    "    \"Sushi Tokyo\",\n",
    "    \"Coffee Shop Paris\",\n",
    "    # Add more places here...\n",
    "]\n",
    "\n",
    "# Collect data\n",
    "all_dataframes_list = []\n",
    "for place in places_to_search:\n",
    "    df = collect_places_for_query(place)\n",
    "    if df is not None:\n",
    "        all_dataframes_list.append(df)\n",
    "\n",
    "if all_dataframes_list:\n",
    "    collected_data_df = pd.concat(all_dataframes_list, ignore_index=True)\n",
    "    print(\"\\n‚úÖ Batch collection complete!\")\n",
    "    print(f\"üìä Total places collected: {len(collected_data_df)}\")\n",
    "    display(collected_data_df.head())\n",
    "else:\n",
    "    print(\"‚ùå No data collected\")\n",
    "    collected_data_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 6: Define BigQuery Upload Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigquery_client() -> Optional[bigquery.Client]:\n",
    "    \"\"\"\n",
    "    Creates BigQuery client using Colab Secrets.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get credentials JSON from Colab Secrets\n",
    "        credentials_json = userdata.get(BIGQUERY_KEY_SECRET)\n",
    "        credentials_dict = json.loads(credentials_json)\n",
    "        \n",
    "        # Create credentials object\n",
    "        credentials = service_account.Credentials.from_service_account_info(\n",
    "            credentials_dict,\n",
    "            scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    "        )\n",
    "        \n",
    "        # Create BigQuery client\n",
    "        client = bigquery.Client(credentials=credentials, project=PROJECT_ID)\n",
    "        print(f\"‚úÖ Connected to BigQuery project: {PROJECT_ID}\")\n",
    "        return client\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating BigQuery client: {e}\")\n",
    "        print(f\"   Make sure '{BIGQUERY_KEY_SECRET}' is set in Colab Secrets\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def upload_to_bigquery(df: pd.DataFrame, table_id: str = None) -> bool:\n",
    "    \"\"\"\n",
    "    Uploads DataFrame to BigQuery.\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ö†Ô∏è Cannot upload empty DataFrame\")\n",
    "        return False\n",
    "\n",
    "    client = get_bigquery_client()\n",
    "    if not client:\n",
    "        return False\n",
    "\n",
    "    table_id = table_id or f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_APPEND\",  # Append to existing table\n",
    "        autodetect=True,  # Auto-detect schema\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        print(f\"\\n‚è≥ Uploading {len(df)} rows to BigQuery...\")\n",
    "        print(f\"   Table: {table_id}\")\n",
    "        \n",
    "        job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "        job.result()  # Wait for upload to complete\n",
    "\n",
    "        print(f\"\\n‚úÖ Successfully uploaded {len(df)} rows!\")\n",
    "        print(f\"   View data at: https://console.cloud.google.com/bigquery?project={PROJECT_ID}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Upload error: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "print(\"‚úÖ BigQuery functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Step 7: Upload to BigQuery\n",
    "\n",
    "Upload the collected data to BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to BigQuery\n",
    "if collected_data_df is not None and not collected_data_df.empty:\n",
    "    print(\"=\"*60)\n",
    "    print(\"‚òÅÔ∏è  UPLOADING TO BIGQUERY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    success = upload_to_bigquery(collected_data_df)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nüéâ Upload complete! Your data is now in BigQuery.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Upload failed. Check the error messages above.\")\n",
    "else:\n",
    "    print(\"‚ùå No data to upload. Run Step 5 first to collect data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 8 (Optional): Save to CSV\n",
    "\n",
    "Download the data as a CSV file instead of or in addition to uploading to BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV file\n",
    "if collected_data_df is not None and not collected_data_df.empty:\n",
    "    filename = \"map_locations_data.csv\"\n",
    "    collected_data_df.to_csv(filename, index=False)\n",
    "    print(f\"‚úÖ Data saved to: {filename}\")\n",
    "    print(f\"üìä Rows: {len(collected_data_df)} | Columns: {len(collected_data_df.columns)}\")\n",
    "    \n",
    "    # Download the file\n",
    "    from google.colab import files\n",
    "    files.download(filename)\n",
    "    print(f\"‚¨áÔ∏è Download started for {filename}\")\n",
    "else:\n",
    "    print(\"‚ùå No data to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 9 (Optional): View Data Summary\n",
    "\n",
    "Get a detailed summary of the collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data summary\n",
    "if collected_data_df is not None and not collected_data_df.empty:\n",
    "    print(\"=\"*60)\n",
    "    print(\"üìä DATA SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nüìà Shape: {collected_data_df.shape[0]} rows √ó {collected_data_df.shape[1]} columns\")\n",
    "    \n",
    "    print(\"\\nüìã Column names:\")\n",
    "    for i, col in enumerate(collected_data_df.columns, 1):\n",
    "        print(f\"   {i}. {col}\")\n",
    "    \n",
    "    print(\"\\nüî¢ Data types:\")\n",
    "    print(collected_data_df.dtypes)\n",
    "    \n",
    "    print(\"\\nüìä Full data preview:\")\n",
    "    display(collected_data_df)\n",
    "else:\n",
    "    print(\"‚ùå No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Done!\n",
    "\n",
    "### What you did:\n",
    "1. ‚úÖ Collected location data from RapidAPI\n",
    "2. ‚úÖ Processed and normalized the data\n",
    "3. ‚úÖ Uploaded to BigQuery (or saved as CSV)\n",
    "\n",
    "### Next steps:\n",
    "- Run Step 5 again to collect more data\n",
    "- Query your data in BigQuery\n",
    "- Analyze your location data!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
