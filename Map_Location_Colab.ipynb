{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üó∫Ô∏è Map Location Data Collector - Google Colab\n",
    "\n",
    "This notebook fetches location data from RapidAPI and uploads it to Google BigQuery.\n",
    "\n",
    "## Features:\n",
    "- üîç Search for places using RapidAPI Google Maps API\n",
    "- üíæ Save data to BigQuery or CSV\n",
    "- üìä Interactive and batch processing modes\n",
    "- üöÄ In-memory caching for efficient API usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q requests pandas google-cloud-bigquery google-auth db-dtypes\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import Optional, Dict, Any, List\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "from google.colab import userdata\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# In-memory cache\n",
    "API_CACHE: Dict[str, Any] = {}\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Step 3: Configure API Credentials\n",
    "\n",
    "### Option A: Using Colab Secrets (Recommended)\n",
    "1. Click on the üîë key icon in the left sidebar\n",
    "2. Add a secret named `RAPIDAPI_KEY` with your API key\n",
    "3. Add a secret named `BIGQUERY_CREDENTIALS` with your service account JSON\n",
    "\n",
    "### Option B: Manual Configuration\n",
    "Uncomment and fill in the credentials below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get credentials from Colab secrets first\n",
    "try:\n",
    "    RAPIDAPI_KEY = userdata.get('RAPIDAPI_KEY')\n",
    "    print(\"‚úÖ RapidAPI key loaded from Colab secrets\")\n",
    "except:\n",
    "    # Manual configuration - uncomment and fill in\n",
    "    RAPIDAPI_KEY = \"ac0025f410mshd0c260cb60f3db6p18c4b0jsnc9b7413cd574\"  # Your API key\n",
    "    print(\"‚úÖ RapidAPI key loaded from manual configuration\")\n",
    "\n",
    "# BigQuery Configuration\n",
    "PROJECT_ID = \"shopper-reviews-477306\"\n",
    "DATASET_ID = \"place_data\"\n",
    "TABLE_ID = \"Map_location\"\n",
    "\n",
    "# BigQuery credentials JSON\n",
    "BIGQUERY_CREDENTIALS = {\n",
    "    \"type\": \"service_account\",\n",
    "    \"project_id\": \"shopper-reviews-477306\",\n",
    "    \"private_key_id\": \"679b00310997262ff77901f080075b509eb9c770\",\n",
    "    \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCPrVXmepJWx8A8\\nXLqDARbLqqmgPwQ4NEmCCOmAZ019aFToc0Yho0/hDyMhRhsW6z/5h8YVEbheb2oR\\nmFK6/v3UEN1Mf6oJWag9pBngM6IO96QAzozjXjCmIVYJku1HWi+7b4mX7La8p77N\\n5fJdOh30ceC6cJSDA51r2xGJDmchRPNhRR8CS9u3xAeZZeB/pgShwJcLM4WY4L3P\\niwc7qkQb91NPbB2/p3hL/JJAtCvVKf61xlWGOKEGW3pIwBUUcF2/OJ3FTuWrY7P8\\n1c/Kz9LUYOZpztK9zjFCNcnCQvvVAow9bqg3fw6xqE172dQT1FG6AieFSCyUib5B\\nXxwNu0phAgMBAAECggEAET1ThPqIxqA54RmgnjQqP7k0Q0XBxDCvRUq7zIFuBdyC\\nm6Wr8OtUnAT3Snh2qv2tSSFRKO6zDaRsDhJrPYQigX3zNR5Nu8jQlseIUfjqusWy\\nHbqq+GPb4y3gJ06Zk/8uolyUHkZJTZe0cvuNZOxNSIBwM6QV3dE4OVx+3SV88GZ/\\nOkAMCUpPRLJux6vJo+l0Qcfe074qjRYPv3XUaGXyHXeOZXmze/lLF6wsEzZmP1A+\\nE9xZmP4ucM3ybrYi3ipRu6YwuR2mRASLy8VFMtcYCvNZGv6ODkjF2xmpucHwX78S\\nzO3mGFES3Hnknjzoif5sJuBewNSztXJcQqKgtSpDhQKBgQDCS6bYj1VR691J5wxA\\n5/fl2MwY4ALIKqW4RtJyNRBZ7+WDAVkq99R6lz+AmQsb6QyiZ/yTZHSUI61Bjn0p\\nd2MD/fpQle7ZOMyR1gKZk5fE5lvmfA5sK+Aax3dRI7xjPBXJYI4hiCMAxgYdhgtI\\nG1C/Nf6O2HoE/W2qLEnLZadpowKBgQC9Tl+/9Eq9Q/DI74CG78U0+s2aRq19vsXZ\\n+wCIUm54TcN9xw4nPKYbT24nTVwTrOu2bxEgDVmuAqtWlKGad16LqZFTZ2aUaEFC\\ni1HL8UKSy5XmNcum8mrKL5+MvwExcQUSmalE3PEQDRjV65QNld0EbQ6JNz74025z\\nm+3ISpIEKwKBgADf5E1fP8wRmrplbtmv8Z64PhryjzCleH9+2h2nfX5aJRdU3zjh\\nSrSOj7uddL5YazUj8LAdKKUuD+6WnJueLPTspL7OHfgeWFVjuDlGv80kGE/OSSZV\\ngDm+ohvcZFGyCIsSgzFFcprjSU3Ct7RIYzGpJY8xDEOPfHninyZqO7mvAoGAIsog\\ndppikd3Ghmbda+7sgwwEdPHAOHeyzJiARI1BmAJShu7p/vP6YtJ6H+broQIKX4CR\\n2R4a+QusiUDPYh/F1EzZVEaQZ32xYJVR9vTjky6u4ZvJTWkHjxipbag8g+WNVRnA\\nLdOcyaJeihG9J7H+6C1Smoz4manhhoWFcWWi5/kCgYEAssgWnlZCygCjEQ/XDVtZ\\nC8/uelJnMHO93U4yF6Xk61gazKYpXpKjNkD3xfxAyQ3zkBkWo7CXg1env8pT9ld1\\nraWCeCmH/w8i0ww3Cmplks5mXIYPrPPuUCEW5D6B8hIyNC1VIoaOlva8+FgJYPIv\\nC5AqN3hBRDOUbophIQmAe5I=\\n-----END PRIVATE KEY-----\\n\",\n",
    "    \"client_email\": \"demand@shopper-reviews-477306.iam.gserviceaccount.com\",\n",
    "    \"client_id\": \"100956109416744224832\",\n",
    "    \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "    \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "    \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "    \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/demand%40shopper-reviews-477306.iam.gserviceaccount.com\",\n",
    "    \"universe_domain\": \"googleapis.com\"\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Credentials configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 4: Define Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_place_name(place_name: str, api_key: str = None) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetches data for a single query from the RapidAPI.\n",
    "    \n",
    "    Args:\n",
    "        place_name: The place to search for\n",
    "        api_key: RapidAPI key (uses global RAPIDAPI_KEY if not provided)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing place data or None on error\n",
    "    \"\"\"\n",
    "    if place_name in API_CACHE:\n",
    "        logger.info(f\"Loading '{place_name}' from cache\")\n",
    "        return API_CACHE[place_name]\n",
    "\n",
    "    logger.info(f\"Calling API for '{place_name}'\")\n",
    "\n",
    "    api_key = api_key or RAPIDAPI_KEY\n",
    "    API_HOST = \"google-search-master-mega.p.rapidapi.com\"\n",
    "\n",
    "    if not api_key:\n",
    "        logger.error(\"RAPIDAPI_KEY not found\")\n",
    "        return None\n",
    "\n",
    "    url = f\"https://{API_HOST}/maps\"\n",
    "    querystring = {\"q\": place_name, \"hl\": \"en\", \"page\": \"1\"}\n",
    "    headers = {\"x-rapidapi-key\": api_key, \"x-rapidapi-host\": API_HOST}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=querystring, timeout=10)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            API_CACHE[place_name] = data\n",
    "            logger.info(f\"Successfully fetched data for '{place_name}'\")\n",
    "            return data\n",
    "        else:\n",
    "            logger.error(f\"API returned status code {response.status_code}\")\n",
    "            logger.error(f\"Response: {response.text}\")\n",
    "            return None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Request error for '{place_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def collect_places_for_query(query: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Collects place data for a single query.\n",
    "    \n",
    "    Args:\n",
    "        query: The place name to search for\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with place data or None on error\n",
    "    \"\"\"\n",
    "    results_data = search_by_place_name(query)\n",
    "\n",
    "    if results_data and 'places' in results_data and results_data['places']:\n",
    "        try:\n",
    "            df = pd.json_normalize(results_data['places'])\n",
    "            df['search_query'] = query\n",
    "            logger.info(f\"Collected {len(df)} places for '{query}'\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing data for '{query}': {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        logger.warning(f\"No 'places' found for '{query}'\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def collect_places_from_list(place_names: List[str]) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Collects place data for a list of place names.\n",
    "    \n",
    "    Args:\n",
    "        place_names: List of place names to search for\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all collected place data or None if no data collected\n",
    "    \"\"\"\n",
    "    all_dataframes_list: List[pd.DataFrame] = []\n",
    "\n",
    "    for query in place_names:\n",
    "        query = query.strip()\n",
    "        if query:\n",
    "            df = collect_places_for_query(query)\n",
    "            if df is not None:\n",
    "                all_dataframes_list.append(df)\n",
    "\n",
    "    if not all_dataframes_list:\n",
    "        logger.warning(\"No data was collected\")\n",
    "        return None\n",
    "\n",
    "    return pd.concat(all_dataframes_list, ignore_index=True)\n",
    "\n",
    "\n",
    "def get_bigquery_client() -> Optional[bigquery.Client]:\n",
    "    \"\"\"\n",
    "    Creates and returns a BigQuery client with proper credentials.\n",
    "    \n",
    "    Returns:\n",
    "        BigQuery client or None on error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        credentials = service_account.Credentials.from_service_account_info(\n",
    "            BIGQUERY_CREDENTIALS,\n",
    "            scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    "        )\n",
    "        client = bigquery.Client(credentials=credentials, project=PROJECT_ID)\n",
    "        logger.info(f\"Connected to BigQuery project: {PROJECT_ID}\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating BigQuery client: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def upload_to_bigquery(df: pd.DataFrame, table_id: str = None) -> bool:\n",
    "    \"\"\"\n",
    "    Uploads a DataFrame to BigQuery.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to upload\n",
    "        table_id: Full table ID in format project.dataset.table\n",
    "        \n",
    "    Returns:\n",
    "        True if upload successful, False otherwise\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        logger.warning(\"Cannot upload empty DataFrame\")\n",
    "        return False\n",
    "    \n",
    "    client = get_bigquery_client()\n",
    "    if not client:\n",
    "        return False\n",
    "    \n",
    "    table_id = table_id or f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_APPEND\",\n",
    "        autodetect=True,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Uploading {len(df)} rows to {table_id}\")\n",
    "        job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "        job.result()  # Wait for the job to complete\n",
    "        \n",
    "        logger.info(f\"‚úÖ Successfully uploaded {len(df)} rows to {table_id}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error uploading to BigQuery: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame, output_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Saves DataFrame to CSV file.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        output_path: Path to save CSV file\n",
    "        \n",
    "    Returns:\n",
    "        True if save successful, False otherwise\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        logger.warning(\"Cannot save empty DataFrame\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        df.to_csv(output_path, index=False)\n",
    "        logger.info(f\"‚úÖ Data saved to {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving to CSV: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"‚úÖ All functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 5: Usage Examples\n",
    "\n",
    "### Option 1: Search for a Single Place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Search for restaurants in New York\n",
    "query = \"restaurants in New York\"\n",
    "\n",
    "df = collect_places_for_query(query)\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"\\n‚úÖ Found {len(df)} places for '{query}'\")\n",
    "    print(\"\\nFirst 5 results:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Optionally save to CSV\n",
    "    # save_to_csv(df, \"single_query_results.csv\")\n",
    "    \n",
    "    # Optionally upload to BigQuery\n",
    "    # upload_to_bigquery(df)\n",
    "else:\n",
    "    print(\"‚ùå No data found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Batch Search for Multiple Places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your list of places to search\n",
    "place_names = [\n",
    "    \"coffee shops in San Francisco\",\n",
    "    \"hotels in Los Angeles\",\n",
    "    \"museums in Chicago\"\n",
    "]\n",
    "\n",
    "print(f\"üîç Searching for {len(place_names)} locations...\\n\")\n",
    "\n",
    "df = collect_places_from_list(place_names)\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"\\n‚úÖ Collected {len(df)} total places\")\n",
    "    print(f\"\\nüìä Data Summary:\")\n",
    "    print(df['search_query'].value_counts())\n",
    "    print(\"\\nFirst 5 results:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Save to CSV\n",
    "    # save_to_csv(df, \"batch_results.csv\")\n",
    "else:\n",
    "    print(\"‚ùå No data collected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Upload Results to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the DataFrame to BigQuery\n",
    "# Make sure you have a DataFrame named 'df' from the previous steps\n",
    "\n",
    "if 'df' in locals() and df is not None:\n",
    "    print(f\"Uploading {len(df)} rows to BigQuery...\")\n",
    "    success = upload_to_bigquery(df)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"\\n‚úÖ Data successfully uploaded to BigQuery!\")\n",
    "        print(f\"Table: {PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\")\n",
    "    else:\n",
    "        print(\"‚ùå Upload failed\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data to upload. Please run a search first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 4: Interactive Search (Input-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive search - enter places one by one\n",
    "all_results = []\n",
    "\n",
    "print(\"üîç Interactive Place Search\")\n",
    "print(\"Enter place names to search (or 'done' to finish)\\n\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"Enter place name: \").strip()\n",
    "    \n",
    "    if query.lower() in ['done', 'exit', 'quit', '']:\n",
    "        break\n",
    "    \n",
    "    df = collect_places_for_query(query)\n",
    "    if df is not None:\n",
    "        all_results.append(df)\n",
    "        print(f\"‚úÖ Found {len(df)} places\\n\")\n",
    "    else:\n",
    "        print(\"‚ùå No results found\\n\")\n",
    "\n",
    "if all_results:\n",
    "    combined_df = pd.concat(all_results, ignore_index=True)\n",
    "    print(f\"\\n‚úÖ Total collected: {len(combined_df)} places\")\n",
    "    display(combined_df.head(10))\n",
    "    \n",
    "    # Optionally upload to BigQuery\n",
    "    upload_choice = input(\"\\nUpload to BigQuery? (yes/no): \").strip().lower()\n",
    "    if upload_choice == 'yes':\n",
    "        upload_to_bigquery(combined_df)\n",
    "else:\n",
    "    print(\"No data collected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Step 6: Download Results as CSV (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the results as CSV\n",
    "from google.colab import files\n",
    "\n",
    "if 'df' in locals() and df is not None:\n",
    "    filename = \"map_location_results.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"‚úÖ CSV file created: {filename}\")\n",
    "    \n",
    "    # Download the file\n",
    "    files.download(filename)\n",
    "    print(\"üì• File downloaded!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data available to download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 7: View Cache Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View cached queries\n",
    "print(f\"üì¶ Cache Status:\")\n",
    "print(f\"Total cached queries: {len(API_CACHE)}\")\n",
    "\n",
    "if API_CACHE:\n",
    "    print(\"\\nCached queries:\")\n",
    "    for query in API_CACHE.keys():\n",
    "        print(f\"  - {query}\")\n",
    "else:\n",
    "    print(\"Cache is empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Step 8: Clear Cache (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the API cache\n",
    "API_CACHE.clear()\n",
    "print(\"‚úÖ Cache cleared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Additional Information\n",
    "\n",
    "### API Information:\n",
    "- **API Provider**: RapidAPI - Google Search Master Mega\n",
    "- **Endpoint**: `/maps`\n",
    "- **Rate Limits**: Check your RapidAPI subscription\n",
    "\n",
    "### BigQuery Table Schema:\n",
    "- Automatically detected from the API response\n",
    "- Includes place name, address, ratings, reviews, and more\n",
    "- Additional field: `search_query` (the original search term)\n",
    "\n",
    "### Tips:\n",
    "1. Use specific search queries for better results (e.g., \"Italian restaurants in Manhattan\")\n",
    "2. The cache prevents duplicate API calls for the same query\n",
    "3. BigQuery uploads append data (no duplicates removed)\n",
    "4. Save intermediate results to CSV as backup\n",
    "\n",
    "### Troubleshooting:\n",
    "- **API errors**: Check your RapidAPI key and subscription status\n",
    "- **BigQuery errors**: Verify credentials and project permissions\n",
    "- **Empty results**: Try different search terms or check API response format\n",
    "\n",
    "---\n",
    "\n",
    "**Created for Google Colab** | Last updated: 2025-11-05"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Map_Location_Colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
