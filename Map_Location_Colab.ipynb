{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üó∫Ô∏è Map Location Data Collector - Google Colab\n",
    "\n",
    "This notebook fetches location data from RapidAPI and uploads it to Google BigQuery.\n",
    "\n",
    "## Features:\n",
    "- üîç Search for places using RapidAPI Google Maps API\n",
    "- üíæ Save data to BigQuery or CSV\n",
    "- üìä Interactive and batch processing modes\n",
    "- üöÄ In-memory caching for efficient API usage\n",
    "- ‚ú® Automatic table creation on first run, append on subsequent runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q requests pandas google-cloud-bigquery google-auth db-dtypes\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import Optional, Dict, Any, List\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "from google.colab import userdata\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# In-memory cache\n",
    "API_CACHE: Dict[str, Any] = {}\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Step 3: Configure API Credentials\n",
    "\n",
    "### Option A: Using Colab Secrets (Recommended)\n",
    "1. Click on the üîë key icon in the left sidebar\n",
    "2. Add a secret named `RAPIDAPI_KEY` with your API key\n",
    "3. Add a secret named `BIGQUERY_CREDENTIALS` with your service account JSON\n",
    "\n",
    "### Option B: Manual Configuration\n",
    "Uncomment and fill in the credentials below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get credentials from Colab secrets first\n",
    "try:\n",
    "    RAPIDAPI_KEY = userdata.get('RAPIDAPI_KEY')\n",
    "    print(\"‚úÖ RapidAPI key loaded from Colab secrets\")\n",
    "except:\n",
    "    # Manual configuration - uncomment and fill in\n",
    "    RAPIDAPI_KEY = \"ac0025f410mshd0c260cb60f3db6p18c4b0jsnc9b7413cd574\"  # Your API key\n",
    "    print(\"‚úÖ RapidAPI key loaded from manual configuration\")\n",
    "\n",
    "# BigQuery Configuration\n",
    "PROJECT_ID = \"shopper-reviews-477306\"\n",
    "DATASET_ID = \"place_data\"\n",
    "TABLE_ID = \"Map_location\"\n",
    "\n",
    "# BigQuery credentials JSON\n",
    "BIGQUERY_CREDENTIALS = {\n",
    "    \"type\": \"service_account\",\n",
    "    \"project_id\": \"shopper-reviews-477306\",\n",
    "    \"private_key_id\": \"679b00310997262ff77901f080075b509eb9c770\",\n",
    "    \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCPrVXmepJWx8A8\\nXLqDARbLqqmgPwQ4NEmCCOmAZ019aFToc0Yho0/hDyMhRhsW6z/5h8YVEbheb2oR\\nmFK6/v3UEN1Mf6oJWag9pBngM6IO96QAzozjXjCmIVYJku1HWi+7b4mX7La8p77N\\n5fJdOh30ceC6cJSDA51r2xGJDmchRPNhRR8CS9u3xAeZZeB/pgShwJcLM4WY4L3P\\niwc7qkQb91NPbB2/p3hL/JJAtCvVKf61xlWGOKEGW3pIwBUUcF2/OJ3FTuWrY7P8\\n1c/Kz9LUYOZpztK9zjFCNcnCQvvVAow9bqg3fw6xqE172dQT1FG6AieFSCyUib5B\\nXxwNu0phAgMBAAECggEAET1ThPqIxqA54RmgnjQqP7k0Q0XBxDCvRUq7zIFuBdyC\\nm6Wr8OtUnAT3Snh2qv2tSSFRKO6zDaRsDhJrPYQigX3zNR5Nu8jQlseIUfjqusWy\\nHbqq+GPb4y3gJ06Zk/8uolyUHkZJTZe0cvuNZOxNSIBwM6QV3dE4OVx+3SV88GZ/\\nOkAMCUpPRLJux6vJo+l0Qcfe074qjRYPv3XUaGXyHXeOZXmze/lLF6wsEzZmP1A+\\nE9xZmP4ucM3ybrYi3ipRu6YwuR2mRASLy8VFMtcYCvNZGv6ODkjF2xmpucHwX78S\\nzO3mGFES3Hnknjzoif5sJuBewNSztXJcQqKgtSpDhQKBgQDCS6bYj1VR691J5wxA\\n5/fl2MwY4ALIKqW4RtJyNRBZ7+WDAVkq99R6lz+AmQsb6QyiZ/yTZHSUI61Bjn0p\\nd2MD/fpQle7ZOMyR1gKZk5fE5lvmfA5sK+Aax3dRI7xjPBXJYI4hiCMAxgYdhgtI\\nG1C/Nf6O2HoE/W2qLEnLZadpowKBgQC9Tl+/9Eq9Q/DI74CG78U0+s2aRq19vsXZ\\n+wCIUm54TcN9xw4nPKYbT24nTVwTrOu2bxEgDVmuAqtWlKGad16LqZFTZ2aUaEFC\\ni1HL8UKSy5XmNcum8mrKL5+MvwExcQUSmalE3PEQDRjV65QNld0EbQ6JNz74025z\\nm+3ISpIEKwKBgADf5E1fP8wRmrplbtmv8Z64PhryjzCleH9+2h2nfX5aJRdU3zjh\\nSrSOj7uddL5YazUj8LAdKKUuD+6WnJueLPTspL7OHfgeWFVjuDlGv80kGE/OSSZV\\ngDm+ohvcZFGyCIsSgzFFcprjSU3Ct7RIYzGpJY8xDEOPfHninyZqO7mvAoGAIsog\\ndppikd3Ghmbda+7sgwwEdPHAOHeyzJiARI1BmAJShu7p/vP6YtJ6H+broQIKX4CR\\n2R4a+QusiUDPYh/F1EzZVEaQZ32xYJVR9vTjky6u4ZvJTWkHjxipbag8g+WNVRnA\\nLdOcyaJeihG9J7H+6C1Smoz4manhhoWFcWWi5/kCgYEAssgWnlZCygCjEQ/XDVtZ\\nC8/uelJnMHO93U4yF6Xk61gazKYpXpKjNkD3xfxAyQ3zkBkWo7CXg1env8pT9ld1\\nraWCeCmH/w8i0ww3Cmplks5mXIYPrPPuUCEW5D6B8hIyNC1VIoaOlva8+FgJYPIv\\nC5AqN3hBRDOUbophIQmAe5I=\\n-----END PRIVATE KEY-----\\n\",\n",
    "    \"client_email\": \"demand@shopper-reviews-477306.iam.gserviceaccount.com\",\n",
    "    \"client_id\": \"100956109416744224832\",\n",
    "    \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "    \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "    \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "    \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/demand%40shopper-reviews-477306.iam.gserviceaccount.com\",\n",
    "    \"universe_domain\": \"googleapis.com\"\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Credentials configured successfully!\")\n",
    "print(f\"üìä Target Table: {PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 4: Define Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_place_name(place_name: str, api_key: str = None) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetches data for a single query from the RapidAPI.\n",
    "    \n",
    "    Args:\n",
    "        place_name: The place to search for\n",
    "        api_key: RapidAPI key (uses global RAPIDAPI_KEY if not provided)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing place data or None on error\n",
    "    \"\"\"\n",
    "    if place_name in API_CACHE:\n",
    "        logger.info(f\"Loading '{place_name}' from cache\")\n",
    "        return API_CACHE[place_name]\n",
    "\n",
    "    logger.info(f\"Calling API for '{place_name}'\")\n",
    "\n",
    "    api_key = api_key or RAPIDAPI_KEY\n",
    "    API_HOST = \"google-search-master-mega.p.rapidapi.com\"\n",
    "\n",
    "    if not api_key:\n",
    "        logger.error(\"RAPIDAPI_KEY not found\")\n",
    "        return None\n",
    "\n",
    "    url = f\"https://{API_HOST}/maps\"\n",
    "    querystring = {\"q\": place_name, \"hl\": \"en\", \"page\": \"1\"}\n",
    "    headers = {\"x-rapidapi-key\": api_key, \"x-rapidapi-host\": API_HOST}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=querystring, timeout=10)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            API_CACHE[place_name] = data\n",
    "            logger.info(f\"Successfully fetched data for '{place_name}'\")\n",
    "            return data\n",
    "        else:\n",
    "            logger.error(f\"API returned status code {response.status_code}\")\n",
    "            logger.error(f\"Response: {response.text}\")\n",
    "            return None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Request error for '{place_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def collect_places_for_query(query: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Collects place data for a single query.\n",
    "    \n",
    "    Args:\n",
    "        query: The place name to search for\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with place data or None on error\n",
    "    \"\"\"\n",
    "    results_data = search_by_place_name(query)\n",
    "\n",
    "    if results_data and 'places' in results_data and results_data['places']:\n",
    "        try:\n",
    "            df = pd.json_normalize(results_data['places'])\n",
    "            df['search_query'] = query\n",
    "            logger.info(f\"Collected {len(df)} places for '{query}'\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing data for '{query}': {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        logger.warning(f\"No 'places' found for '{query}'\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def collect_places_from_list(place_names: List[str]) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Collects place data for a list of place names.\n",
    "    \n",
    "    Args:\n",
    "        place_names: List of place names to search for\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all collected place data or None if no data collected\n",
    "    \"\"\"\n",
    "    all_dataframes_list: List[pd.DataFrame] = []\n",
    "\n",
    "    for query in place_names:\n",
    "        query = query.strip()\n",
    "        if query:\n",
    "            df = collect_places_for_query(query)\n",
    "            if df is not None:\n",
    "                all_dataframes_list.append(df)\n",
    "\n",
    "    if not all_dataframes_list:\n",
    "        logger.warning(\"No data was collected\")\n",
    "        return None\n",
    "\n",
    "    return pd.concat(all_dataframes_list, ignore_index=True)\n",
    "\n",
    "\n",
    "def combine_opening_hours(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combines all openingHours columns into a single JSON string column.\n",
    "    \n",
    "    Finds columns like 'openingHours.Monday', 'openingHours.Tuesday', etc.\n",
    "    and combines them into a single 'openingHours' column as a JSON string.\n",
    "    Also cleans Unicode characters for better readability.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with potentially separate openingHours columns\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with combined openingHours column\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Find all columns that start with 'openingHours.'\n",
    "    opening_hours_cols = [col for col in df_copy.columns if col.startswith('openingHours.')]\n",
    "    \n",
    "    if opening_hours_cols:\n",
    "        logger.info(f\"Combining {len(opening_hours_cols)} openingHours columns into one\")\n",
    "        \n",
    "        def clean_hours_text(text):\n",
    "            \"\"\"Clean Unicode characters from opening hours text\"\"\"\n",
    "            if not isinstance(text, str):\n",
    "                return text\n",
    "            \n",
    "            # Replace Unicode characters with standard equivalents\n",
    "            text = text.replace('\\u202f', ' ')      # Narrow no-break space ‚Üí regular space\n",
    "            text = text.replace('\\u2013', '-')      # En dash ‚Üí hyphen\n",
    "            text = text.replace('\\u2014', '-')      # Em dash ‚Üí hyphen\n",
    "            text = text.replace('\\xa0', ' ')        # Non-breaking space ‚Üí regular space\n",
    "            text = text.replace('\\u2009', ' ')      # Thin space ‚Üí regular space\n",
    "            \n",
    "            # Remove multiple spaces\n",
    "            text = ' '.join(text.split())\n",
    "            \n",
    "            return text\n",
    "        \n",
    "        # Create a new column with dictionary of all opening hours\n",
    "        def combine_hours_row(row):\n",
    "            hours_dict = {}\n",
    "            for col in opening_hours_cols:\n",
    "                # Extract day name (e.g., 'Monday' from 'openingHours.Monday')\n",
    "                day = col.replace('openingHours.', '')\n",
    "                value = row[col]\n",
    "                # Only add if not null/empty\n",
    "                if pd.notna(value) and value != '':\n",
    "                    # Clean the value\n",
    "                    cleaned_value = clean_hours_text(value)\n",
    "                    hours_dict[day] = cleaned_value\n",
    "            # Return as JSON string for BigQuery compatibility\n",
    "            return json.dumps(hours_dict, ensure_ascii=False) if hours_dict else None\n",
    "        \n",
    "        # Create the combined column\n",
    "        df_copy['openingHours'] = df_copy.apply(combine_hours_row, axis=1)\n",
    "        \n",
    "        # Drop the individual columns\n",
    "        df_copy = df_copy.drop(columns=opening_hours_cols)\n",
    "        \n",
    "        logger.info(f\"‚úÖ Combined openingHours columns into single JSON column\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def sanitize_column_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sanitizes DataFrame column names to be BigQuery-compatible.\n",
    "    \n",
    "    BigQuery column names must:\n",
    "    - Contain only letters, numbers, and underscores\n",
    "    - Start with a letter or underscore\n",
    "    - Be at most 300 characters long\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with potentially invalid column names\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with sanitized column names\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    new_columns = {}\n",
    "    for col in df.columns:\n",
    "        # Replace dots, spaces, and other special characters with underscores\n",
    "        sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', col)\n",
    "        \n",
    "        # Ensure it doesn't start with a number\n",
    "        if sanitized and sanitized[0].isdigit():\n",
    "            sanitized = '_' + sanitized\n",
    "        \n",
    "        # Ensure it's not empty\n",
    "        if not sanitized:\n",
    "            sanitized = 'column_' + str(df.columns.get_loc(col))\n",
    "        \n",
    "        # Limit to 300 characters\n",
    "        sanitized = sanitized[:300]\n",
    "        \n",
    "        # Handle duplicates by appending number\n",
    "        if sanitized in new_columns.values():\n",
    "            counter = 1\n",
    "            while f\"{sanitized}_{counter}\" in new_columns.values():\n",
    "                counter += 1\n",
    "            sanitized = f\"{sanitized}_{counter}\"\n",
    "        \n",
    "        new_columns[col] = sanitized\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    df_copy.columns = [new_columns[col] for col in df.columns]\n",
    "    \n",
    "    logger.info(f\"Sanitized {len([c for c in df.columns if c != new_columns[c]])} column names for BigQuery compatibility\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def get_bigquery_client() -> Optional[bigquery.Client]:\n",
    "    \"\"\"\n",
    "    Creates and returns a BigQuery client with proper credentials.\n",
    "    \n",
    "    Returns:\n",
    "        BigQuery client or None on error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        credentials = service_account.Credentials.from_service_account_info(\n",
    "            BIGQUERY_CREDENTIALS,\n",
    "            scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    "        )\n",
    "        client = bigquery.Client(credentials=credentials, project=PROJECT_ID)\n",
    "        logger.info(f\"Connected to BigQuery project: {PROJECT_ID}\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating BigQuery client: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def check_table_exists(table_id: str = None) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a BigQuery table exists.\n",
    "    \n",
    "    Args:\n",
    "        table_id: Full table ID in format project.dataset.table\n",
    "        \n",
    "    Returns:\n",
    "        True if table exists, False otherwise\n",
    "    \"\"\"\n",
    "    client = get_bigquery_client()\n",
    "    if not client:\n",
    "        return False\n",
    "    \n",
    "    table_id = table_id or f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
    "    \n",
    "    try:\n",
    "        client.get_table(table_id)\n",
    "        logger.info(f\"‚úÖ Table {table_id} exists\")\n",
    "        return True\n",
    "    except Exception:\n",
    "        logger.info(f\"‚ö†Ô∏è Table {table_id} does not exist\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_existing_place_ids(table_id: str = None) -> set:\n",
    "    \"\"\"\n",
    "    Retrieves all existing place IDs from BigQuery table.\n",
    "    \n",
    "    Args:\n",
    "        table_id: Full table ID in format project.dataset.table\n",
    "        \n",
    "    Returns:\n",
    "        Set of existing place IDs, empty set if table doesn't exist or on error\n",
    "    \"\"\"\n",
    "    client = get_bigquery_client()\n",
    "    if not client:\n",
    "        return set()\n",
    "    \n",
    "    table_id = table_id or f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
    "    \n",
    "    # Check if table exists first\n",
    "    if not check_table_exists(table_id):\n",
    "        logger.info(\"Table doesn't exist yet, no existing place IDs to check\")\n",
    "        return set()\n",
    "    \n",
    "    try:\n",
    "        # Query to get all place IDs (try common field names)\n",
    "        # Try place_id first, then placeId, then id\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT \n",
    "            COALESCE(place_id, placeId, id) as place_id\n",
    "        FROM `{table_id}`\n",
    "        WHERE COALESCE(place_id, placeId, id) IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        result = client.query(query).result()\n",
    "        existing_ids = {row.place_id for row in result}\n",
    "        \n",
    "        logger.info(f\"Found {len(existing_ids)} existing place IDs in table\")\n",
    "        return existing_ids\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not retrieve existing place IDs: {e}\")\n",
    "        logger.info(\"Proceeding without deduplication check\")\n",
    "        return set()\n",
    "\n",
    "\n",
    "def remove_duplicate_places(df: pd.DataFrame, table_id: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes rows with place IDs that already exist in BigQuery.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with place data\n",
    "        table_id: Full table ID in format project.dataset.table\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with duplicate places removed\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    \n",
    "    # Find place_id column (could be place_id, placeId, or id)\n",
    "    place_id_col = None\n",
    "    for col in ['place_id', 'placeId', 'id']:\n",
    "        if col in df.columns:\n",
    "            place_id_col = col\n",
    "            break\n",
    "    \n",
    "    if place_id_col is None:\n",
    "        logger.warning(\"No place_id column found in data, skipping deduplication\")\n",
    "        return df\n",
    "    \n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Get existing place IDs from BigQuery\n",
    "    existing_ids = get_existing_place_ids(table_id)\n",
    "    \n",
    "    if not existing_ids:\n",
    "        logger.info(\"No existing place IDs to check, uploading all records\")\n",
    "        return df\n",
    "    \n",
    "    # Filter out rows with existing place IDs\n",
    "    df_filtered = df[~df[place_id_col].isin(existing_ids)].copy()\n",
    "    \n",
    "    duplicates_removed = original_count - len(df_filtered)\n",
    "    \n",
    "    if duplicates_removed > 0:\n",
    "        logger.info(f\"üîç Removed {duplicates_removed} duplicate place(s) that already exist\")\n",
    "        logger.info(f\"üì§ {len(df_filtered)} new place(s) to upload\")\n",
    "    else:\n",
    "        logger.info(f\"‚úÖ All {original_count} place(s) are new\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def create_bigquery_table(table_id: str = None, schema: List[bigquery.SchemaField] = None) -> bool:\n",
    "    \"\"\"\n",
    "    Creates a new BigQuery table.\n",
    "    \n",
    "    Args:\n",
    "        table_id: Full table ID in format project.dataset.table\n",
    "        schema: List of SchemaField objects (optional, will auto-detect if not provided)\n",
    "        \n",
    "    Returns:\n",
    "        True if creation successful, False otherwise\n",
    "    \"\"\"\n",
    "    client = get_bigquery_client()\n",
    "    if not client:\n",
    "        return False\n",
    "    \n",
    "    table_id = table_id or f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
    "    \n",
    "    try:\n",
    "        # Check if table already exists\n",
    "        if check_table_exists(table_id):\n",
    "            logger.info(f\"Table {table_id} already exists, skipping creation\")\n",
    "            return True\n",
    "        \n",
    "        # Create table object\n",
    "        table = bigquery.Table(table_id, schema=schema)\n",
    "        \n",
    "        # Create the table\n",
    "        table = client.create_table(table)\n",
    "        logger.info(f\"‚úÖ Created table {table_id}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating table: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def upload_to_bigquery(df: pd.DataFrame, table_id: str = None, create_if_needed: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Uploads a DataFrame to BigQuery.\n",
    "    Creates the table on first run, then appends on subsequent runs.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to upload\n",
    "        table_id: Full table ID in format project.dataset.table\n",
    "        create_if_needed: If True, creates table if it doesn't exist\n",
    "        \n",
    "    Returns:\n",
    "        True if upload successful, False otherwise\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        logger.warning(\"Cannot upload empty DataFrame\")\n",
    "        return False\n",
    "    \n",
    "    client = get_bigquery_client()\n",
    "    if not client:\n",
    "        return False\n",
    "    \n",
    "    table_id = table_id or f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
    "    \n",
    "    # Combine openingHours columns into one\n",
    "    df = combine_opening_hours(df)\n",
    "    \n",
    "    # Sanitize column names for BigQuery compatibility\n",
    "    df = sanitize_column_names(df)\n",
    "    \n",
    "    # Check if table exists\n",
    "    table_exists = check_table_exists(table_id)\n",
    "    \n",
    "    # Remove duplicates if table exists\n",
    "    if table_exists:\n",
    "        df = remove_duplicate_places(df, table_id)\n",
    "        \n",
    "        # If all records are duplicates, nothing to upload\n",
    "        if df.empty:\n",
    "            logger.info(\"‚ö†Ô∏è All records already exist in BigQuery. Nothing to upload.\")\n",
    "            return True\n",
    "    \n",
    "    if not table_exists:\n",
    "        if create_if_needed:\n",
    "            logger.info(f\"Table does not exist. Creating table {table_id}...\")\n",
    "            # First, create table with schema from first batch of data\n",
    "            job_config = bigquery.LoadJobConfig(\n",
    "                write_disposition=\"WRITE_TRUNCATE\",  # Create new table\n",
    "                autodetect=True,  # Auto-detect schema\n",
    "            )\n",
    "        else:\n",
    "            logger.error(f\"Table {table_id} does not exist and create_if_needed=False\")\n",
    "            return False\n",
    "    else:\n",
    "        logger.info(f\"Table exists. Appending data to {table_id}...\")\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            write_disposition=\"WRITE_APPEND\",  # Append to existing table\n",
    "            autodetect=False,  # Use existing schema\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Uploading {len(df)} rows to {table_id}\")\n",
    "        job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "        job.result()  # Wait for the job to complete\n",
    "        \n",
    "        if not table_exists and create_if_needed:\n",
    "            logger.info(f\"‚úÖ Successfully created table and uploaded {len(df)} rows to {table_id}\")\n",
    "        else:\n",
    "            logger.info(f\"‚úÖ Successfully appended {len(df)} rows to {table_id}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error uploading to BigQuery: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame, output_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Saves DataFrame to CSV file.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        output_path: Path to save CSV file\n",
    "        \n",
    "    Returns:\n",
    "        True if save successful, False otherwise\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        logger.warning(\"Cannot save empty DataFrame\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        df.to_csv(output_path, index=False)\n",
    "        logger.info(f\"‚úÖ Data saved to {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving to CSV: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"‚úÖ All functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 5: Check BigQuery Table Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the Map_location table exists\n",
    "table_name = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
    "print(f\"Checking table: {table_name}\")\n",
    "print()\n",
    "\n",
    "exists = check_table_exists()\n",
    "\n",
    "if exists:\n",
    "    print(f\"\\n‚úÖ Table '{TABLE_ID}' exists!\")\n",
    "    print(\"Future uploads will APPEND data to this table.\")\n",
    "    \n",
    "    # Get table info\n",
    "    client = get_bigquery_client()\n",
    "    if client:\n",
    "        table = client.get_table(table_name)\n",
    "        print(f\"\\nüìä Table Info:\")\n",
    "        print(f\"  - Total rows: {table.num_rows:,}\")\n",
    "        print(f\"  - Created: {table.created}\")\n",
    "        print(f\"  - Modified: {table.modified}\")\n",
    "        print(f\"  - Size: {table.num_bytes / (1024*1024):.2f} MB\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Table '{TABLE_ID}' does NOT exist yet.\")\n",
    "    print(\"It will be created automatically on first data upload.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 6: Usage Examples\n",
    "\n",
    "### Option 1: Search for a Single Place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Search for restaurants in New York\n",
    "query = \"restaurants in New York\"\n",
    "\n",
    "df = collect_places_for_query(query)\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"\\n‚úÖ Found {len(df)} places for '{query}'\")\n",
    "    print(\"\\nFirst 5 results:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Optionally save to CSV\n",
    "    # save_to_csv(df, \"single_query_results.csv\")\n",
    "    \n",
    "    # Optionally upload to BigQuery\n",
    "    # upload_to_bigquery(df)\n",
    "else:\n",
    "    print(\"‚ùå No data found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Batch Search for Multiple Places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your list of places to search\n",
    "place_names = [\n",
    "    \"coffee shops in San Francisco\",\n",
    "    \"hotels in Los Angeles\",\n",
    "    \"museums in Chicago\"\n",
    "]\n",
    "\n",
    "print(f\"üîç Searching for {len(place_names)} locations...\\n\")\n",
    "\n",
    "df = collect_places_from_list(place_names)\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"\\n‚úÖ Collected {len(df)} total places\")\n",
    "    print(f\"\\nüìä Data Summary:\")\n",
    "    print(df['search_query'].value_counts())\n",
    "    print(\"\\nFirst 5 results:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Save to CSV\n",
    "    # save_to_csv(df, \"batch_results.csv\")\n",
    "else:\n",
    "    print(\"‚ùå No data collected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Upload Results to BigQuery (Creates Table or Appends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the DataFrame to BigQuery\n",
    "# This will CREATE the table on first run, then APPEND on subsequent runs\n",
    "\n",
    "if 'df' in locals() and df is not None:\n",
    "    print(f\"üì§ Uploading {len(df)} rows to BigQuery...\\n\")\n",
    "    \n",
    "    # Check if table exists before upload\n",
    "    exists_before = check_table_exists()\n",
    "    print()\n",
    "    \n",
    "    # Upload (will create table if needed, or append if it exists)\n",
    "    success = upload_to_bigquery(df)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"\\n‚úÖ Upload successful!\")\n",
    "        print(f\"\\nüìä Table: {PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\")\n",
    "        \n",
    "        if not exists_before:\n",
    "            print(\"\\nüéâ Table was CREATED with this upload (first time)\")\n",
    "            print(\"Future uploads will APPEND to this table.\")\n",
    "        else:\n",
    "            print(\"\\nüìù Data was APPENDED to existing table\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Upload failed. Check logs above for details.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data to upload. Please run a search first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 4: Interactive Search (Input-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive search - enter places one by one\n",
    "all_results = []\n",
    "\n",
    "print(\"üîç Interactive Place Search\")\n",
    "print(\"Enter place names to search (or 'done' to finish)\\n\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"Enter place name: \").strip()\n",
    "    \n",
    "    if query.lower() in ['done', 'exit', 'quit', '']:\n",
    "        break\n",
    "    \n",
    "    df = collect_places_for_query(query)\n",
    "    if df is not None:\n",
    "        all_results.append(df)\n",
    "        print(f\"‚úÖ Found {len(df)} places\\n\")\n",
    "    else:\n",
    "        print(\"‚ùå No results found\\n\")\n",
    "\n",
    "if all_results:\n",
    "    combined_df = pd.concat(all_results, ignore_index=True)\n",
    "    print(f\"\\n‚úÖ Total collected: {len(combined_df)} places\")\n",
    "    display(combined_df.head(10))\n",
    "    \n",
    "    # Optionally upload to BigQuery\n",
    "    upload_choice = input(\"\\nUpload to BigQuery? (yes/no): \").strip().lower()\n",
    "    if upload_choice == 'yes':\n",
    "        upload_to_bigquery(combined_df)\n",
    "else:\n",
    "    print(\"No data collected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Step 7: Download Results as CSV (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the results as CSV\n",
    "from google.colab import files\n",
    "\n",
    "if 'df' in locals() and df is not None:\n",
    "    filename = \"map_location_results.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"‚úÖ CSV file created: {filename}\")\n",
    "    \n",
    "    # Download the file\n",
    "    files.download(filename)\n",
    "    print(\"üì• File downloaded!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data available to download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 8: Query BigQuery Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the BigQuery table to see what's stored\n",
    "client = get_bigquery_client()\n",
    "\n",
    "if client and check_table_exists():\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        search_query,\n",
    "        COUNT(*) as place_count\n",
    "    FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\n",
    "    GROUP BY search_query\n",
    "    ORDER BY place_count DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üìä Querying BigQuery table...\\n\")\n",
    "    \n",
    "    try:\n",
    "        result_df = client.query(query).to_dataframe()\n",
    "        print(f\"‚úÖ Query successful! Found {len(result_df)} unique searches\\n\")\n",
    "        display(result_df)\n",
    "        \n",
    "        print(f\"\\nüìà Total places in table: {result_df['place_count'].sum():,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Query failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Table does not exist yet. Upload data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 9: View Cache Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View cached queries\n",
    "print(f\"üì¶ Cache Status:\")\n",
    "print(f\"Total cached queries: {len(API_CACHE)}\")\n",
    "\n",
    "if API_CACHE:\n",
    "    print(\"\\nCached queries:\")\n",
    "    for query in API_CACHE.keys():\n",
    "        print(f\"  - {query}\")\n",
    "else:\n",
    "    print(\"Cache is empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Step 10: Clear Cache (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the API cache\n",
    "API_CACHE.clear()\n",
    "print(\"‚úÖ Cache cleared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Additional Information\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "#### **First Run (Table Creation):**\n",
    "1. Run Step 6 to collect data\n",
    "2. Run Step 6 Option 3 to upload - **Table will be CREATED**\n",
    "3. Schema is auto-detected from your data\n",
    "4. Table: `shopper-reviews-477306.place_data.Map_location`\n",
    "\n",
    "#### **Subsequent Runs (Append Data):**\n",
    "1. Collect more data with new searches\n",
    "2. Upload again - **Data will be APPENDED**\n",
    "3. No duplicates are removed (manual deduplication needed if required)\n",
    "\n",
    "### API Information:\n",
    "- **API Provider**: RapidAPI - Google Search Master Mega\n",
    "- **Endpoint**: `/maps`\n",
    "- **Rate Limits**: Check your RapidAPI subscription\n",
    "\n",
    "### BigQuery Table Schema (Auto-detected):\n",
    "Common fields include:\n",
    "- `title` - Place name\n",
    "- `address` - Full address\n",
    "- `rating` - Average rating\n",
    "- `reviews` - Number of reviews\n",
    "- `openingHours` - Combined opening hours as JSON (e.g., {\"Monday\": \"9 AM-5 PM\", \"Tuesday\": \"9 AM-5 PM\"})\n",
    "- `search_query` - Original search term (added by script)\n",
    "- And many more fields from the API response\n",
    "\n",
    "**Automatic Data Processing:**\n",
    "1. **Opening Hours Combination**: All `openingHours.Monday`, `openingHours.Tuesday`, etc. columns are automatically combined into a single `openingHours` column as a JSON string with clean formatting\n",
    "2. **Unicode Character Cleaning**: Special characters (\\u202f, \\u2013, etc.) are replaced with standard spaces and hyphens\n",
    "3. **Column Name Sanitization**: Special characters (dots, spaces, etc.) are replaced with underscores\n",
    "4. **Duplicate Prevention**: Before uploading, checks existing `place_id` values in BigQuery and skips duplicates (only uploads new places)\n",
    "5. This ensures clean, organized, and unique data in BigQuery\n",
    "\n",
    "### Tips:\n",
    "1. Use specific search queries for better results\n",
    "2. The cache prevents duplicate API calls for the same query\n",
    "3. Check table status with Step 5 before uploading\n",
    "4. Query your data with Step 8 to see what's stored\n",
    "5. Save intermediate results to CSV as backup\n",
    "6. **No need to worry about duplicates!** The system automatically checks for existing `place_id` values and only uploads new places\n",
    "\n",
    "### Troubleshooting:\n",
    "- **API errors**: Check your RapidAPI key and subscription status\n",
    "- **BigQuery errors**: Verify credentials and project permissions\n",
    "- **Empty results**: Try different search terms\n",
    "- **Schema errors**: On first upload, ensure your data is clean\n",
    "\n",
    "---\n",
    "\n",
    "**Created for Google Colab** | Last updated: 2025-11-05"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Map_Location_Colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
