{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåü Google Reviews Fetcher - FLATTENED with Deduplication\n",
    "\n",
    "Fetches reviews from Google Reviews API and stores in BigQuery.\n",
    "\n",
    "## ‚ú® Features:\n",
    "- üóÇÔ∏è **FLATTENED**: Each review = One row\n",
    "- üîë **Unique review_id**: Prevents duplicates\n",
    "- üö´ **Auto-deduplication**: Skips existing reviews\n",
    "- üîÑ **Pagination**: Fetches all pages\n",
    "- üìä **Easy queries**: Standard SQL columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-cloud-bigquery google-auth pandas db-dtypes\n",
    "print(\"‚úÖ Packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import hashlib\n",
    "import http.client\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "from typing import Optional, Dict, Any, List\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "from google.colab import userdata\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Step 3: Configure Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    RAPIDAPI_KEY = userdata.get('RAPIDAPI_KEY')\n",
    "    print(\"‚úÖ RapidAPI key loaded from secrets\")\n",
    "except:\n",
    "    RAPIDAPI_KEY = \"ac0025f410mshd0c260cb60f3db6p18c4b0jsnc9b7413cd574\"\n",
    "    print(\"‚ö†Ô∏è Using hardcoded RapidAPI key\")\n",
    "\n",
    "try:\n",
    "    BIGQUERY_CREDENTIALS_STR = userdata.get('BIGQUERY_KEY_JSON')\n",
    "    BIGQUERY_CREDENTIALS = json.loads(BIGQUERY_CREDENTIALS_STR)\n",
    "    print(\"‚úÖ BigQuery credentials from secrets\")\n",
    "    PROJECT_ID = BIGQUERY_CREDENTIALS.get('project_id', 'shopper-reviews-477306')\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Using hardcoded BigQuery credentials\")\n",
    "    PROJECT_ID = \"shopper-reviews-477306\"\n",
    "    BIGQUERY_CREDENTIALS = {\n",
    "        \"type\": \"service_account\",\n",
    "        \"project_id\": \"shopper-reviews-477306\",\n",
    "        \"private_key_id\": \"679b00310997262ff77901f080075b509eb9c770\",\n",
    "        \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCPrVXmepJWx8A8\\nXLqDARbLqqmgPwQ4NEmCCOmAZ019aFToc0Yho0/hDyMhRhsW6z/5h8YVEbheb2oR\\nmFK6/v3UEN1Mf6oJWag9pBngM6IO96QAzozjXjCmIVYJku1HWi+7b4mX7La8p77N\\n5fJdOh30ceC6cJSDA51r2xGJDmchRPNhRR8CS9u3xAeZZeB/pgShwJcLM4WY4L3P\\niwc7qkQb91NPbB2/p3hL/JJAtCvVKf61xlWGOKEGW3pIwBUUcF2/OJ3FTuWrY7P8\\n1c/Kz9LUYOZpztK9zjFCNcnCQvvVAow9bqg3fw6xqE172dQT1FG6AieFSCyUib5B\\nXxwNu0phAgMBAAECggEAET1ThPqIxqA54RmgnjQqP7k0Q0XBxDCvRUq7zIFuBdyC\\nm6Wr8OtUnAT3Snh2qv2tSSFRKO6zDaRsDhJrPYQigX3zNR5Nu8jQlseIUfjqusWy\\nHbqq+GPb4y3gJ06Zk/8uolyUHkZJTZe0cvuNZOxNSIBwM6QV3dE4OVx+3SV88GZ/\\nOkAMCUpPRLJux6vJo+l0Qcfe074qjRYPv3XUaGXyHXeOZXmze/lLF6wsEzZmP1A+\\nE9xZmP4ucM3ybrYi3ipRu6YwuR2mRASLy8VFMtcYCvNZGv6ODkjF2xmpucHwX78S\\nzO3mGFES3Hnknjzoif5sJuBewNSztXJcQqKgtSpDhQKBgQDCS6bYj1VR691J5wxA\\n5/fl2MwY4ALIKqW4RtJyNRBZ7+WDAVkq99R6lz+AmQsb6QyiZ/yTZHSUI61Bjn0p\\nd2MD/fpQle7ZOMyR1gKZk5fE5lvmfA5sK+Aax3dRI7xjPBXJYI4hiCMAxgYdhgtI\\nG1C/Nf6O2HoE/W2qLEnLZadpowKBgQC9Tl+/9Eq9Q/DI74CG78U0+s2aRq19vsXZ\\n+wCIUm54TcN9xw4nPKYbT24nTVwTrOu2bxEgDVmuAqtWlKGad16LqZFTZ2aUaEFC\\ni1HL8UKSy5XmNcum8mrKL5+MvwExcQUSmalE3PEQDRjV65QNld0EbQ6JNz74025z\\nm+3ISpIEKwKBgADf5E1fP8wRmrplbtmv8Z64PhryjzCleH9+2h2nfX5aJRdU3zjh\\nSrSOj7uddL5YazUj8LAdKKUuD+6WnJueLPTspL7OHfgeWFVjuDlGv80kGE/OSSZV\\ngDm+ohvcZFGyCIsSgzFFcprjSU3Ct7RIYzGpJY8xDEOPfHninyZqO7mvAoGAIsog\\ndppikd3Ghmbda+7sgwwEdPHAOHeyzJiARI1BmAJShu7p/vP6YtJ6H+broQIKX4CR\\n2R4a+QusiUDPYh/F1EzZVEaQZ32xYJVR9vTjky6u4ZvJTWkHjxipbag8g+WNVRnA\\nLdOcyaJeihG9J7H+6C1Smoz4manhhoWFcWWi5/kCgYEAssgWnlZCygCjEQ/XDVtZ\\nC8/uelJnMHO93U4yF6Xk61gazKYpXpKjNkD3xfxAyQ3zkBkWo7CXg1env8pT9ld1\\nraWCeCmH/w8i0ww3Cmplks5mXIYPrPPuUCEW5D6B8hIyNC1VIoaOlva8+FgJYPIv\\nC5AqN3hBRDOUbophIQmAe5I=\\n-----END PRIVATE KEY-----\\n\",\n",
    "        \"client_email\": \"demand@shopper-reviews-477306.iam.gserviceaccount.com\",\n",
    "        \"client_id\": \"100956109416744224832\",\n",
    "        \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "        \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "        \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "        \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/demand%40shopper-reviews-477306.iam.gserviceaccount.com\",\n",
    "        \"universe_domain\": \"googleapis.com\"\n",
    "    }\n",
    "\n",
    "DATASET_ID = \"place_data\"\n",
    "SOURCE_TABLE = \"Map_location\"\n",
    "DESTINATION_TABLE = \"place_reviews_full\"\n",
    "API_HOST = \"google-search-master-mega.p.rapidapi.com\"\n",
    "MAX_PAGES = None  # None = fetch ALL pages (no limit), or set number for safety\n",
    "RETRY_ATTEMPTS = 3\n",
    "RETRY_DELAY = 2\n",
    "\n",
    "print(f\"\\n‚úÖ Configuration loaded!\")\n",
    "print(f\"üìä Source: {PROJECT_ID}.{DATASET_ID}.{SOURCE_TABLE}\")\n",
    "print(f\"üìä Destination: {PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\")\n",
    "print(f\"üîë Schema: FLATTENED with review_id (no duplicates!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 4: Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigquery_client() -> Optional[bigquery.Client]:\n",
    "    try:\n",
    "        credentials = service_account.Credentials.from_service_account_info(\n",
    "            BIGQUERY_CREDENTIALS,\n",
    "            scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    "        )\n",
    "        client = bigquery.Client(credentials=credentials, project=PROJECT_ID)\n",
    "        logger.info(f\"‚úÖ Connected to BigQuery: {PROJECT_ID}\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå BigQuery error: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_reviews_for_place(place_id: str, page: int = 1) -> Optional[Dict[str, Any]]:\n",
    "    for attempt in range(RETRY_ATTEMPTS):\n",
    "        try:\n",
    "            conn = http.client.HTTPSConnection(API_HOST)\n",
    "            headers = {'x-rapidapi-key': RAPIDAPI_KEY, 'x-rapidapi-host': API_HOST}\n",
    "            params = f\"?cid={place_id}&sortBy=mostRelevant&gl=us&hl=en&page={page}\"\n",
    "            \n",
    "            logger.info(f\"üì° Fetching page {page} for CID {place_id}...\")\n",
    "            \n",
    "            conn.request(\"GET\", \"/reviews\" + params, headers=headers)\n",
    "            res = conn.getresponse()\n",
    "            data = res.read()\n",
    "            \n",
    "            if res.status == 200:\n",
    "                result = json.loads(data.decode(\"utf-8\"))\n",
    "                logger.info(f\"‚úÖ Page {page} fetched\")\n",
    "                return result\n",
    "            else:\n",
    "                logger.warning(f\"‚ö†Ô∏è API status {res.status}\")\n",
    "                if attempt < RETRY_ATTEMPTS - 1:\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error: {e}\")\n",
    "            if attempt < RETRY_ATTEMPTS - 1:\n",
    "                time.sleep(RETRY_DELAY)\n",
    "    return None\n",
    "\n",
    "def fetch_all_reviews_for_place(place_id: str) -> Dict[str, Any]:\n",
    "    all_reviews = []\n",
    "    page = 1\n",
    "    \n",
    "    logger.info(f\"üîç Fetching reviews for CID {place_id}...\")\n",
    "    \n",
    "    while page <= MAX_PAGES:\n",
    "        result = fetch_reviews_for_place(place_id, page)\n",
    "        if not result:\n",
    "            break\n",
    "        \n",
    "        reviews = result.get('reviews', [])\n",
    "        all_reviews.extend(reviews)\n",
    "        logger.info(f\"‚úÖ Page {page}: {len(reviews)} reviews\")\n",
    "        \n",
    "        if not result.get('nextPageToken') or len(reviews) == 0:\n",
    "            break\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    logger.info(f\"üéâ Total: {len(all_reviews)} reviews\")\n",
    "    return {'place_id': place_id, 'total_reviews': len(all_reviews), 'reviews': all_reviews, 'pages_fetched': page}\n",
    "\n",
    "def generate_review_id(place_id: str, iso_date: str, reviewer_name: str, snippet: str) -> str:\n",
    "    \"\"\"Generates unique review ID using hash.\"\"\"\n",
    "    unique_string = f\"{place_id}_{iso_date}_{reviewer_name}_{snippet[:100]}\"\n",
    "    review_id = hashlib.sha256(unique_string.encode('utf-8')).hexdigest()[:16]\n",
    "    return review_id\n",
    "\n",
    "def flatten_reviews_to_rows(review_data: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Flattens reviews with unique review_id for each row.\"\"\"\n",
    "    place_id = review_data['place_id']\n",
    "    reviews = review_data['reviews']\n",
    "    current_time = datetime.now(timezone.utc)\n",
    "    current_date = current_time.date()\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    for review in reviews:\n",
    "        user = review.get('user', {})\n",
    "        iso_date = review.get('isoDate', '')\n",
    "        \n",
    "        try:\n",
    "            iso_timestamp = datetime.fromisoformat(iso_date.replace('Z', '+00:00')) if iso_date else None\n",
    "        except:\n",
    "            iso_timestamp = None\n",
    "        \n",
    "        reviewer_name = user.get('name', '')\n",
    "        snippet = review.get('snippet', '')\n",
    "        \n",
    "        # Generate unique review_id\n",
    "        review_id = generate_review_id(place_id, iso_date, reviewer_name, snippet)\n",
    "        \n",
    "        row = {\n",
    "            'review_id': review_id,\n",
    "            'place_id': place_id,\n",
    "            'rating': review.get('rating'),\n",
    "            'date': review.get('date'),\n",
    "            'isoDate': iso_timestamp,\n",
    "            'snippet': snippet,\n",
    "            'likes': review.get('likes'),\n",
    "            'reviewer_name': reviewer_name,\n",
    "            'reviewer_link': user.get('link'),\n",
    "            'reviewer_thumbnail': user.get('thumbnail'),\n",
    "            'reviewer_reviews': user.get('reviews'),\n",
    "            'reviewer_photos': user.get('photos'),\n",
    "            'timestamp': current_time,\n",
    "            'fetch_date': current_date,\n",
    "        }\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    logger.info(f\"‚úÖ Flattened {len(rows)} reviews with unique review_ids\")\n",
    "    return df\n",
    "\n",
    "def get_existing_review_ids(client: bigquery.Client) -> set:\n",
    "    \"\"\"Gets existing review_ids to prevent duplicates.\"\"\"\n",
    "    table_id = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
    "    \n",
    "    try:\n",
    "        client.get_table(table_id)\n",
    "        query = f\"SELECT DISTINCT review_id FROM `{table_id}` WHERE review_id IS NOT NULL\"\n",
    "        result = client.query(query).to_dataframe()\n",
    "        existing_ids = set(result['review_id'].tolist())\n",
    "        logger.info(f\"üìä Found {len(existing_ids)} existing review IDs\")\n",
    "        return existing_ids\n",
    "    except Exception:\n",
    "        logger.info(\"No existing reviews found\")\n",
    "        return set()\n",
    "\n",
    "def remove_duplicate_reviews(df: pd.DataFrame, client: bigquery.Client) -> pd.DataFrame:\n",
    "    \"\"\"Removes reviews that already exist in BigQuery.\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    original_count = len(df)\n",
    "    existing_ids = get_existing_review_ids(client)\n",
    "    \n",
    "    if not existing_ids:\n",
    "        logger.info(\"‚úÖ No existing reviews, uploading all\")\n",
    "        return df\n",
    "    \n",
    "    df_filtered = df[~df['review_id'].isin(existing_ids)].copy()\n",
    "    duplicates_removed = original_count - len(df_filtered)\n",
    "    \n",
    "    if duplicates_removed > 0:\n",
    "        logger.info(f\"üîç Removed {duplicates_removed} duplicate(s)\")\n",
    "        logger.info(f\"üì§ {len(df_filtered)} new review(s)\")\n",
    "    else:\n",
    "        logger.info(f\"‚úÖ All {original_count} review(s) are new\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "def create_reviews_table_if_not_exists(client: bigquery.Client) -> bool:\n",
    "    \"\"\"Creates table with review_id as primary key.\"\"\"\n",
    "    table_id = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
    "    \n",
    "    try:\n",
    "        try:\n",
    "            client.get_table(table_id)\n",
    "            logger.info(f\"‚úÖ Table exists: {DESTINATION_TABLE}\")\n",
    "            return True\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"review_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField(\"place_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField(\"rating\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"date\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"isoDate\", \"TIMESTAMP\"),\n",
    "            bigquery.SchemaField(\"snippet\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"likes\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"reviewer_name\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"reviewer_link\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"reviewer_thumbnail\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"reviewer_reviews\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"reviewer_photos\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"timestamp\", \"TIMESTAMP\"),\n",
    "            bigquery.SchemaField(\"fetch_date\", \"DATE\"),\n",
    "        ]\n",
    "        \n",
    "        table = bigquery.Table(table_id, schema=schema)\n",
    "        client.create_table(table)\n",
    "        \n",
    "        logger.info(f\"‚úÖ Created table: {DESTINATION_TABLE} (with review_id)\")\n",
    "        print(f\"\\nüîë Schema includes review_id to prevent duplicates!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Table creation error: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_place_ids_to_process(client: bigquery.Client, limit: int = None) -> List[str]:\n",
    "    \"\"\"Gets CIDs from Map_location.\"\"\"\n",
    "    source_table = f\"{PROJECT_ID}.{DATASET_ID}.{SOURCE_TABLE}\"\n",
    "    \n",
    "    try:\n",
    "        dest_table = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
    "        \n",
    "        try:\n",
    "            client.get_table(dest_table)\n",
    "            query = f\"\"\"SELECT DISTINCT cid as place_id FROM `{source_table}` WHERE cid IS NOT NULL\n",
    "            AND cid NOT IN (SELECT DISTINCT place_id FROM `{dest_table}` WHERE place_id IS NOT NULL)\"\"\"\n",
    "            if limit:\n",
    "                query += f\" LIMIT {limit}\"\n",
    "            logger.info(\"üìä Reading 'cid' column...\")\n",
    "        except:\n",
    "            query = f\"SELECT DISTINCT cid as place_id FROM `{source_table}` WHERE cid IS NOT NULL\"\n",
    "            if limit:\n",
    "                query += f\" LIMIT {limit}\"\n",
    "            logger.info(\"üìä Reading all CIDs...\")\n",
    "        \n",
    "        result = client.query(query).to_dataframe()\n",
    "        place_ids = result['place_id'].tolist()\n",
    "        logger.info(f\"‚úÖ Found {len(place_ids)} CID(s)\")\n",
    "        return place_ids\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error fetching CIDs: {e}\")\n",
    "        return []\n",
    "\n",
    "def upload_review_data_to_bigquery(client: bigquery.Client, review_data: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Uploads reviews with automatic deduplication.\"\"\"\n",
    "    table_id = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
    "    \n",
    "    try:\n",
    "        df = flatten_reviews_to_rows(review_data)\n",
    "        if df.empty:\n",
    "            logger.warning(\"No reviews\")\n",
    "            return False\n",
    "        \n",
    "        df = remove_duplicate_reviews(df, client)\n",
    "        if df.empty:\n",
    "            logger.info(\"‚ö†Ô∏è All reviews already exist, skipping\")\n",
    "            return True\n",
    "        \n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_APPEND\")\n",
    "        logger.info(f\"Uploading {len(df)} new review(s)...\")\n",
    "        job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "        job.result()\n",
    "        \n",
    "        logger.info(f\"‚úÖ Uploaded {len(df)} new review(s)\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Upload error: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"‚úÖ All functions defined!\")\n",
    "print(\"\\nüîë Key Features:\")\n",
    "print(\"  ‚Ä¢ Unique review_id for each review\")\n",
    "print(\"  ‚Ä¢ Automatic deduplication (no duplicate reviews!)\")\n",
    "print(\"  ‚Ä¢ Flattened schema (easy SQL queries)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 5: Check Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_bigquery_client()\n",
    "\n",
    "if client:\n",
    "    print(\"üìä Current Status:\\n\")\n",
    "    \n",
    "    source_table = f\"{PROJECT_ID}.{DATASET_ID}.{SOURCE_TABLE}\"\n",
    "    try:\n",
    "        table = client.get_table(source_table)\n",
    "        print(f\"‚úÖ Source: {SOURCE_TABLE}\")\n",
    "        print(f\"   Total rows: {table.num_rows:,}\")\n",
    "        \n",
    "        query = f\"SELECT COUNT(DISTINCT cid) as count FROM `{source_table}` WHERE cid IS NOT NULL\"\n",
    "        result = client.query(query).to_dataframe()\n",
    "        print(f\"   Places with CID: {result['count'].iloc[0]:,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Source error: {e}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    dest_table = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
    "    try:\n",
    "        table = client.get_table(dest_table)\n",
    "        print(f\"‚úÖ Destination: {DESTINATION_TABLE}\")\n",
    "        print(f\"   Schema: FLATTENED with review_id\")\n",
    "        print(f\"   Total review rows: {table.num_rows:,}\")\n",
    "        \n",
    "        query = f\"\"\"SELECT \n",
    "            COUNT(DISTINCT place_id) as places,\n",
    "            COUNT(DISTINCT review_id) as unique_reviews,\n",
    "            COUNT(*) as total_rows,\n",
    "            AVG(rating) as avg_rating\n",
    "        FROM `{dest_table}`\"\"\"\n",
    "        result = client.query(query).to_dataframe()\n",
    "        print(f\"   Places: {result['places'].iloc[0]:,}\")\n",
    "        print(f\"   Unique reviews: {result['unique_reviews'].iloc[0]:,}\")\n",
    "        print(f\"   Total rows: {result['total_rows'].iloc[0]:,}\")\n",
    "        print(f\"   Avg rating: {result['avg_rating'].iloc[0]:.2f} ‚≠ê\")\n",
    "        \n",
    "        duplicates = result['total_rows'].iloc[0] - result['unique_reviews'].iloc[0]\n",
    "        if duplicates > 0:\n",
    "            print(f\"   ‚ö†Ô∏è Duplicates: {duplicates}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ No duplicates!\")\n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è Destination doesn't exist (will be created)\")\n",
    "        print(f\"   Will use: FLATTENED schema with review_id\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to connect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 6: Test Single Place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_place_id = \"7632417579134624850\"\n",
    "\n",
    "print(f\"üß™ Testing CID: {test_place_id}\\n\")\n",
    "\n",
    "review_data = fetch_all_reviews_for_place(test_place_id)\n",
    "\n",
    "print(f\"\\nüìä API Results:\")\n",
    "print(f\"Place ID: {review_data['place_id']}\")\n",
    "print(f\"Total Reviews: {review_data['total_reviews']}\")\n",
    "print(f\"Pages Fetched: {review_data['pages_fetched']}\")\n",
    "\n",
    "df_flattened = flatten_reviews_to_rows(review_data)\n",
    "\n",
    "print(f\"\\nüóÇÔ∏è Flattened Data:\")\n",
    "print(f\"Rows: {len(df_flattened)} (one per review)\")\n",
    "print(f\"Unique review_ids: {df_flattened['review_id'].nunique()}\")\n",
    "\n",
    "print(f\"\\nüìã Preview:\")\n",
    "display(df_flattened[['review_id', 'rating', 'reviewer_name', 'snippet']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì§ Step 7: Upload Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_bigquery_client()\n",
    "\n",
    "if client and 'review_data' in locals():\n",
    "    print(\"üì§ Uploading test data...\\n\")\n",
    "    \n",
    "    if create_reviews_table_if_not_exists(client):\n",
    "        if upload_review_data_to_bigquery(client, review_data):\n",
    "            print(\"\\n‚úÖ Test data uploaded!\")\n",
    "            print(f\"üîë Duplicates will be automatically prevented by review_id\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå Upload failed\")\n",
    "else:\n",
    "    print(\"‚ùå No client or data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Step 8: Batch Process (All Places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_bigquery_client()\n",
    "\n",
    "if not client:\n",
    "    print(\"‚ùå No client\")\n",
    "else:\n",
    "    print(\"üöÄ Starting batch processing...\\n\")\n",
    "    \n",
    "    if not create_reviews_table_if_not_exists(client):\n",
    "        print(\"‚ùå Table creation failed\")\n",
    "    else:\n",
    "        place_ids = get_place_ids_to_process(client, limit=5)\n",
    "        \n",
    "        if not place_ids:\n",
    "            print(\"‚úÖ No new places!\")\n",
    "        else:\n",
    "            print(f\"üìä Processing {len(place_ids)} place(s)...\\n\")\n",
    "            \n",
    "            successful = 0\n",
    "            failed = 0\n",
    "            skipped = 0\n",
    "            total_new_reviews = 0\n",
    "            total_duplicates = 0\n",
    "            \n",
    "            for idx, place_id in enumerate(place_ids, 1):\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"üìç Place {idx}/{len(place_ids)}: {place_id}\")\n",
    "                print(f\"{'='*60}\")\n",
    "                \n",
    "                try:\n",
    "                    review_data = fetch_all_reviews_for_place(place_id)\n",
    "                    \n",
    "                    if review_data['total_reviews'] == 0:\n",
    "                        print(f\"‚ö†Ô∏è No reviews, skipping\")\n",
    "                        skipped += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Track duplicates\n",
    "                    df = flatten_reviews_to_rows(review_data)\n",
    "                    df_new = remove_duplicate_reviews(df, client)\n",
    "                    duplicates_found = len(df) - len(df_new)\n",
    "                    total_duplicates += duplicates_found\n",
    "                    \n",
    "                    if upload_review_data_to_bigquery(client, review_data):\n",
    "                        successful += 1\n",
    "                        total_new_reviews += len(df_new)\n",
    "                        print(f\"‚úÖ Success: {len(df_new)} new, {duplicates_found} duplicates skipped\")\n",
    "                    else:\n",
    "                        failed += 1\n",
    "                        \n",
    "                except KeyboardInterrupt:\n",
    "                    print(f\"\\n‚ö†Ô∏è Interrupted!\")\n",
    "                    break\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    failed += 1\n",
    "                    print(f\"‚ùå Error: {e}\")\n",
    "                \n",
    "                if idx < len(place_ids):\n",
    "                    time.sleep(1)\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"üìä SUMMARY\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"‚úÖ Successful: {successful}\")\n",
    "            print(f\"‚ùå Failed: {failed}\")\n",
    "            print(f\"‚è≠Ô∏è Skipped: {skipped}\")\n",
    "            print(f\"üìä New Reviews Added: {total_new_reviews:,}\")\n",
    "            print(f\"üîç Duplicates Prevented: {total_duplicates:,}\")\n",
    "            print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 9: Query & Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_bigquery_client()\n",
    "\n",
    "if client:\n",
    "    table_name = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
    "    \n",
    "    try:\n",
    "        print(\"üìä Review Statistics\\n\")\n",
    "        \n",
    "        stats_query = f\"\"\"SELECT \n",
    "            COUNT(DISTINCT place_id) as places,\n",
    "            COUNT(DISTINCT review_id) as unique_reviews,\n",
    "            COUNT(*) as total_rows,\n",
    "            AVG(rating) as avg_rating,\n",
    "            MAX(timestamp) as last_fetch\n",
    "        FROM `{table_name}`\"\"\"\n",
    "        \n",
    "        stats = client.query(stats_query).to_dataframe()\n",
    "        display(stats)\n",
    "        \n",
    "        duplicates = stats['total_rows'].iloc[0] - stats['unique_reviews'].iloc[0]\n",
    "        if duplicates > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è WARNING: {duplicates} duplicate rows detected!\")\n",
    "            print(f\"üí° Run deduplication query to clean up.\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ No duplicates - review_id working perfectly!\")\n",
    "        \n",
    "        print(\"\\nüìà Sample Reviews:\")\n",
    "        sample_query = f\"\"\"SELECT \n",
    "            review_id, place_id, rating, date, reviewer_name,\n",
    "            LEFT(snippet, 80) as snippet_preview\n",
    "        FROM `{table_name}`\n",
    "        ORDER BY timestamp DESC LIMIT 10\"\"\"\n",
    "        \n",
    "        samples = client.query(sample_query).to_dataframe()\n",
    "        display(samples)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Documentation\n",
    "\n",
    "### üîë Key Feature: review_id\n",
    "\n",
    "**What is it?**  \n",
    "A unique 16-character hash generated from:\n",
    "- `place_id`\n",
    "- `isoDate`\n",
    "- `reviewer_name`\n",
    "- `snippet` (first 100 chars)\n",
    "\n",
    "**Why?**  \n",
    "- Same review always gets same ID\n",
    "- Automatic duplicate prevention\n",
    "- Can safely re-run script\n",
    "\n",
    "### üìä Schema:\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| **review_id** | STRING | **Unique review identifier** |\n",
    "| place_id | STRING | Place CID |\n",
    "| rating | INTEGER | 1-5 stars |\n",
    "| date | STRING | Relative date |\n",
    "| isoDate | TIMESTAMP | ISO timestamp |\n",
    "| snippet | STRING | Review text |\n",
    "| likes | INTEGER | Like count |\n",
    "| reviewer_name | STRING | Reviewer name |\n",
    "| reviewer_link | STRING | Profile link |\n",
    "| reviewer_thumbnail | STRING | Image URL |\n",
    "| reviewer_reviews | INTEGER | Total reviews |\n",
    "| reviewer_photos | INTEGER | Total photos |\n",
    "| timestamp | TIMESTAMP | Insert time |\n",
    "| fetch_date | DATE | Fetch date |\n",
    "\n",
    "### üîç Deduplication:\n",
    "\n",
    "The script automatically:\n",
    "1. Generates `review_id` for each review\n",
    "2. Queries existing `review_id`s from BigQuery\n",
    "3. Filters out duplicates before upload\n",
    "4. Reports: `X new, Y duplicates skipped`\n",
    "\n",
    "**Result**: No duplicate reviews! üéâ\n",
    "\n",
    "### Example Query:\n",
    "\n",
    "```sql\n",
    "-- Count reviews per place\n",
    "SELECT \n",
    "    place_id,\n",
    "    COUNT(DISTINCT review_id) as review_count,\n",
    "    AVG(rating) as avg_rating\n",
    "FROM `shopper-reviews-477306.place_data.place_reviews_full`\n",
    "GROUP BY place_id\n",
    "ORDER BY review_count DESC\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Version**: 3.0 - With review_id & Deduplication  \n",
    "**Last Updated**: 2025-11-05"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Review_Fetcher_Colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
