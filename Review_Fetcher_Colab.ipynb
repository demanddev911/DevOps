{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåü Google Reviews Data Fetcher - Google Colab\n",
    "\n",
    "This notebook fetches complete review data from Google Reviews API (via RapidAPI) and stores it in **FLATTENED** BigQuery format.\n",
    "\n",
    "## Features:\n",
    "- üìä Fetches ALL review data from Google Reviews API\n",
    "- üóÇÔ∏è **FLATTENED structure**: Each review = One row with individual columns\n",
    "- üîÑ Automatic pagination (follows nextPageToken)\n",
    "- üíæ Stores in structured BigQuery table (no JSON!)\n",
    "- ‚ö° Incremental processing (only new places)\n",
    "- üõ°Ô∏è Robust error handling and retries\n",
    "- üìà Progress tracking and logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-cloud-bigquery google-auth pandas db-dtypes\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import http.client\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "from typing import Optional, Dict, Any, List\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "from google.colab import userdata\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Step 3: Configure API Credentials\n",
    "\n",
    "### Option A: Using Colab Secrets (Recommended)\n",
    "1. Click on the üîë key icon in the left sidebar\n",
    "2. Add a secret named `RAPIDAPI_KEY` with your API key\n",
    "3. Add a secret named `BIGQUERY_KEY_JSON` with your service account JSON\n",
    "\n",
    "### Option B: Manual Configuration\n",
    "Uncomment and fill in the credentials below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get credentials from Colab secrets first\n",
    "try:\n",
    "    RAPIDAPI_KEY = userdata.get('RAPIDAPI_KEY')\n",
    "    print(\"‚úÖ RapidAPI key loaded from Colab secrets\")\n",
    "except:\n",
    "    # Manual configuration\n",
    "    RAPIDAPI_KEY = \"ac0025f410mshd0c260cb60f3db6p18c4b0jsnc9b7413cd574\"\n",
    "    print(\"‚ö†Ô∏è RapidAPI key loaded from manual configuration\")\n",
    "\n",
    "# Load BigQuery credentials from secrets\n",
    "try:\n",
    "    BIGQUERY_CREDENTIALS_STR = userdata.get('BIGQUERY_KEY_JSON')\n",
    "    BIGQUERY_CREDENTIALS = json.loads(BIGQUERY_CREDENTIALS_STR)\n",
    "    print(\"‚úÖ BigQuery credentials loaded from Colab secrets\")\n",
    "    PROJECT_ID = BIGQUERY_CREDENTIALS.get('project_id', 'shopper-reviews-477306')\n",
    "except:\n",
    "    # Fallback to manual configuration\n",
    "    print(\"‚ö†Ô∏è BigQuery credentials loaded from manual configuration\")\n",
    "    PROJECT_ID = \"shopper-reviews-477306\"\n",
    "    BIGQUERY_CREDENTIALS = {\n",
    "        \"type\": \"service_account\",\n",
    "        \"project_id\": \"shopper-reviews-477306\",\n",
    "        \"private_key_id\": \"679b00310997262ff77901f080075b509eb9c770\",\n",
    "        \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCPrVXmepJWx8A8\\nXLqDARbLqqmgPwQ4NEmCCOmAZ019aFToc0Yho0/hDyMhRhsW6z/5h8YVEbheb2oR\\nmFK6/v3UEN1Mf6oJWag9pBngM6IO96QAzozjXjCmIVYJku1HWi+7b4mX7La8p77N\\n5fJdOh30ceC6cJSDA51r2xGJDmchRPNhRR8CS9u3xAeZZeB/pgShwJcLM4WY4L3P\\niwc7qkQb91NPbB2/p3hL/JJAtCvVKf61xlWGOKEGW3pIwBUUcF2/OJ3FTuWrY7P8\\n1c/Kz9LUYOZpztK9zjFCNcnCQvvVAow9bqg3fw6xqE172dQT1FG6AieFSCyUib5B\\nXxwNu0phAgMBAAECggEAET1ThPqIxqA54RmgnjQqP7k0Q0XBxDCvRUq7zIFuBdyC\\nm6Wr8OtUnAT3Snh2qv2tSSFRKO6zDaRsDhJrPYQigX3zNR5Nu8jQlseIUfjqusWy\\nHbqq+GPb4y3gJ06Zk/8uolyUHkZJTZe0cvuNZOxNSIBwM6QV3dE4OVx+3SV88GZ/\\nOkAMCUpPRLJux6vJo+l0Qcfe074qjRYPv3XUaGXyHXeOZXmze/lLF6wsEzZmP1A+\\nE9xZmP4ucM3ybrYi3ipRu6YwuR2mRASLy8VFMtcYCvNZGv6ODkjF2xmpucHwX78S\\nzO3mGFES3Hnknjzoif5sJuBewNSztXJcQqKgtSpDhQKBgQDCS6bYj1VR691J5wxA\\n5/fl2MwY4ALIKqW4RtJyNRBZ7+WDAVkq99R6lz+AmQsb6QyiZ/yTZHSUI61Bjn0p\\nd2MD/fpQle7ZOMyR1gKZk5fE5lvmfA5sK+Aax3dRI7xjPBXJYI4hiCMAxgYdhgtI\\nG1C/Nf6O2HoE/W2qLEnLZadpowKBgQC9Tl+/9Eq9Q/DI74CG78U0+s2aRq19vsXZ\\n+wCIUm54TcN9xw4nPKYbT24nTVwTrOu2bxEgDVmuAqtWlKGad16LqZFTZ2aUaEFC\\ni1HL8UKSy5XmNcum8mrKL5+MvwExcQUSmalE3PEQDRjV65QNld0EbQ6JNz74025z\\nm+3ISpIEKwKBgADf5E1fP8wRmrplbtmv8Z64PhryjzCleH9+2h2nfX5aJRdU3zjh\\nSrSOj7uddL5YazUj8LAdKKUuD+6WnJueLPTspL7OHfgeWFVjuDlGv80kGE/OSSZV\\ngDm+ohvcZFGyCIsSgzFFcprjSU3Ct7RIYzGpJY8xDEOPfHninyZqO7mvAoGAIsog\\ndppikd3Ghmbda+7sgwwEdPHAOHeyzJiARI1BmAJShu7p/vP6YtJ6H+broQIKX4CR\\n2R4a+QusiUDPYh/F1EzZVEaQZ32xYJVR9vTjky6u4ZvJTWkHjxipbag8g+WNVRnA\\nLdOcyaJeihG9J7H+6C1Smoz4manhhoWFcWWi5/kCgYEAssgWnlZCygCjEQ/XDVtZ\\nC8/uelJnMHO93U4yF6Xk61gazKYpXpKjNkD3xfxAyQ3zkBkWo7CXg1env8pT9ld1\\nraWCeCmH/w8i0ww3Cmplks5mXIYPrPPuUCEW5D6B8hIyNC1VIoaOlva8+FgJYPIv\\nC5AqN3hBRDOUbophIQmAe5I=\\n-----END PRIVATE KEY-----\\n\",\n",
    "        \"client_email\": \"demand@shopper-reviews-477306.iam.gserviceaccount.com\",\n",
    "        \"client_id\": \"100956109416744224832\",\n",
    "        \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "        \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "        \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "        \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/demand%40shopper-reviews-477306.iam.gserviceaccount.com\",\n",
    "        \"universe_domain\": \"googleapis.com\"\n",
    "    }\n",
    "\n",
    "# BigQuery Configuration\n",
    "DATASET_ID = \"place_data\"\n",
    "SOURCE_TABLE = \"Map_location\"  # Source table (reads from 'cid' column)\n",
    "DESTINATION_TABLE = \"place_reviews_full\"  # Table to store FLATTENED reviews\n",
    "\n",
    "# API Configuration\n",
    "API_HOST = \"google-search-master-mega.p.rapidapi.com\"\n",
    "MAX_PAGES = 10  # Maximum pages to fetch per place\n",
    "RETRY_ATTEMPTS = 3\n",
    "RETRY_DELAY = 2  # seconds\n",
    "\n",
    "print(\"\\n‚úÖ All configuration loaded!\")\n",
    "print(f\"üìä Source Table: {PROJECT_ID}.{DATASET_ID}.{SOURCE_TABLE}\")\n",
    "print(f\"üìä Destination Table: {PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\")\n",
    "print(\"\\nüóÇÔ∏è Schema: FLATTENED - Each review = One row\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 4: Define Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== BIGQUERY CLIENT ====================\n",
    "\n",
    "def get_bigquery_client() -> Optional[bigquery.Client]:\n",
    "    \"\"\"Creates and returns a BigQuery client.\"\"\"\n",
    "    try:\n",
    "        credentials = service_account.Credentials.from_service_account_info(\n",
    "            BIGQUERY_CREDENTIALS,\n",
    "            scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    "        )\n",
    "        client = bigquery.Client(credentials=credentials, project=PROJECT_ID)\n",
    "        logger.info(f\"‚úÖ Connected to BigQuery project: {PROJECT_ID}\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error creating BigQuery client: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ==================== API FUNCTIONS ====================\n",
    "\n",
    "def fetch_reviews_for_place(place_id: str, page: int = 1) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Fetches review data for a single page from Google Reviews API.\"\"\"\n",
    "    for attempt in range(RETRY_ATTEMPTS):\n",
    "        try:\n",
    "            conn = http.client.HTTPSConnection(API_HOST)\n",
    "            \n",
    "            headers = {\n",
    "                'x-rapidapi-key': RAPIDAPI_KEY,\n",
    "                'x-rapidapi-host': API_HOST\n",
    "            }\n",
    "            \n",
    "            params = f\"?cid={place_id}&sortBy=mostRelevant&gl=us&hl=en&page={page}\"\n",
    "            endpoint = \"/reviews\" + params\n",
    "            \n",
    "            logger.info(f\"üì° Fetching page {page} for CID {place_id}...\")\n",
    "            \n",
    "            conn.request(\"GET\", endpoint, headers=headers)\n",
    "            res = conn.getresponse()\n",
    "            data = res.read()\n",
    "            \n",
    "            if res.status == 200:\n",
    "                result = json.loads(data.decode(\"utf-8\"))\n",
    "                logger.info(f\"‚úÖ Page {page} fetched successfully\")\n",
    "                return result\n",
    "            else:\n",
    "                logger.warning(f\"‚ö†Ô∏è API status {res.status}, attempt {attempt + 1}/{RETRY_ATTEMPTS}\")\n",
    "                if attempt < RETRY_ATTEMPTS - 1:\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error: {e}, attempt {attempt + 1}/{RETRY_ATTEMPTS}\")\n",
    "            if attempt < RETRY_ATTEMPTS - 1:\n",
    "                time.sleep(RETRY_DELAY)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def fetch_all_reviews_for_place(place_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"Fetches ALL reviews for a place by following pagination.\"\"\"\n",
    "    all_reviews = []\n",
    "    all_topics = []\n",
    "    metadata = {}\n",
    "    page = 1\n",
    "    \n",
    "    logger.info(f\"üîç Fetching all reviews for CID {place_id}...\")\n",
    "    \n",
    "    while page <= MAX_PAGES:\n",
    "        result = fetch_reviews_for_place(place_id, page)\n",
    "        \n",
    "        if not result:\n",
    "            logger.warning(f\"‚ö†Ô∏è No data for page {page}, stopping\")\n",
    "            break\n",
    "        \n",
    "        reviews = result.get('reviews', [])\n",
    "        all_reviews.extend(reviews)\n",
    "        \n",
    "        if page == 1:\n",
    "            all_topics = result.get('topics', [])\n",
    "            metadata = {\n",
    "                'searchParameters': result.get('searchParameters', {}),\n",
    "                'credits': result.get('credits', 0),\n",
    "            }\n",
    "        \n",
    "        logger.info(f\"‚úÖ Page {page}: {len(reviews)} reviews\")\n",
    "        \n",
    "        next_page_token = result.get('nextPageToken')\n",
    "        if not next_page_token or len(reviews) == 0:\n",
    "            logger.info(f\"‚úÖ All pages fetched (stopped at page {page})\")\n",
    "            break\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    logger.info(f\"üéâ Total: {len(all_reviews)} reviews, {len(all_topics)} topics\")\n",
    "    \n",
    "    return {\n",
    "        'place_id': place_id,\n",
    "        'total_reviews': len(all_reviews),\n",
    "        'reviews': all_reviews,\n",
    "        'topics': all_topics,\n",
    "        'metadata': metadata,\n",
    "        'pages_fetched': page\n",
    "    }\n",
    "\n",
    "\n",
    "# ==================== DATA FLATTENING ====================\n",
    "\n",
    "def flatten_reviews_to_rows(review_data: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flattens review data into individual rows.\n",
    "    Each review becomes ONE row with all fields as separate columns.\n",
    "    \n",
    "    Args:\n",
    "        review_data: Dictionary containing review data from API\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with flattened review rows\n",
    "    \"\"\"\n",
    "    place_id = review_data['place_id']\n",
    "    reviews = review_data['reviews']\n",
    "    current_time = datetime.now(timezone.utc)\n",
    "    current_date = current_time.date()\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    for review in reviews:\n",
    "        # Extract user data safely\n",
    "        user = review.get('user', {})\n",
    "        \n",
    "        # Parse ISO date\n",
    "        iso_date = review.get('isoDate')\n",
    "        try:\n",
    "            iso_timestamp = datetime.fromisoformat(iso_date.replace('Z', '+00:00')) if iso_date else None\n",
    "        except:\n",
    "            iso_timestamp = None\n",
    "        \n",
    "        # Create flattened row - each review = one row\n",
    "        row = {\n",
    "            'place_id': place_id,\n",
    "            'rating': review.get('rating'),\n",
    "            'date': review.get('date'),\n",
    "            'isoDate': iso_timestamp,\n",
    "            'snippet': review.get('snippet'),\n",
    "            'likes': review.get('likes'),\n",
    "            'reviewer_name': user.get('name'),\n",
    "            'reviewer_link': user.get('link'),\n",
    "            'reviewer_thumbnail': user.get('thumbnail'),\n",
    "            'reviewer_reviews': user.get('reviews'),\n",
    "            'reviewer_photos': user.get('photos'),\n",
    "            'timestamp': current_time,\n",
    "            'fetch_date': current_date,\n",
    "        }\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    logger.info(f\"‚úÖ Flattened {len(rows)} reviews into individual rows\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ==================== BIGQUERY OPERATIONS ====================\n",
    "\n",
    "def get_place_ids_to_process(client: bigquery.Client, limit: int = None) -> List[str]:\n",
    "    \"\"\"Retrieves CIDs from Map_location table that need reviews fetched.\"\"\"\n",
    "    source_table = f\"{PROJECT_ID}.{DATASET_ID}.{SOURCE_TABLE}\"\n",
    "    \n",
    "    try:\n",
    "        dest_table = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
    "        \n",
    "        try:\n",
    "            client.get_table(dest_table)\n",
    "            # Table exists, exclude already processed places\n",
    "            query = f\"\"\"\n",
    "            SELECT DISTINCT cid as place_id\n",
    "            FROM `{source_table}`\n",
    "            WHERE cid IS NOT NULL\n",
    "            AND cid NOT IN (\n",
    "                SELECT DISTINCT place_id\n",
    "                FROM `{dest_table}`\n",
    "                WHERE place_id IS NOT NULL\n",
    "            )\n",
    "            \"\"\"\n",
    "            if limit:\n",
    "                query += f\" LIMIT {limit}\"\n",
    "            logger.info(\"üìä Reading 'cid' column from Map_location...\")\n",
    "        except:\n",
    "            # Table doesn't exist yet\n",
    "            query = f\"\"\"\n",
    "            SELECT DISTINCT cid as place_id\n",
    "            FROM `{source_table}`\n",
    "            WHERE cid IS NOT NULL\n",
    "            \"\"\"\n",
    "            if limit:\n",
    "                query += f\" LIMIT {limit}\"\n",
    "            logger.info(\"üìä Reading all CIDs from Map_location...\")\n",
    "        \n",
    "        result = client.query(query).to_dataframe()\n",
    "        place_ids = result['place_id'].tolist()\n",
    "        \n",
    "        logger.info(f\"‚úÖ Found {len(place_ids)} CID(s) to process\")\n",
    "        return place_ids\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error fetching CIDs: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def create_reviews_table_if_not_exists(client: bigquery.Client) -> bool:\n",
    "    \"\"\"\n",
    "    Creates FLATTENED reviews table.\n",
    "    Schema: Each review = One row with individual columns.\n",
    "    \"\"\"\n",
    "    table_id = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
    "    \n",
    "    try:\n",
    "        try:\n",
    "            client.get_table(table_id)\n",
    "            logger.info(f\"‚úÖ Table {DESTINATION_TABLE} already exists\")\n",
    "            return True\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # FLATTENED schema - each review is a separate row\n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"place_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField(\"rating\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"date\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"isoDate\", \"TIMESTAMP\"),\n",
    "            bigquery.SchemaField(\"snippet\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"likes\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"reviewer_name\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"reviewer_link\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"reviewer_thumbnail\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"reviewer_reviews\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"reviewer_photos\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"timestamp\", \"TIMESTAMP\"),\n",
    "            bigquery.SchemaField(\"fetch_date\", \"DATE\"),\n",
    "        ]\n",
    "        \n",
    "        table = bigquery.Table(table_id, schema=schema)\n",
    "        table = client.create_table(table)\n",
    "        \n",
    "        logger.info(f\"‚úÖ Created FLATTENED table {DESTINATION_TABLE}\")\n",
    "        print(f\"\\nüóÇÔ∏è Table created: {DESTINATION_TABLE}\")\n",
    "        print(f\"üìã Schema: FLATTENED (Each review = One row)\")\n",
    "        print(f\"üìä Columns: place_id, rating, date, snippet, reviewer_name, etc.\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error creating table: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def upload_review_data_to_bigquery(client: bigquery.Client, review_data: Dict[str, Any]) -> bool:\n",
    "    \"\"\"\n",
    "    Uploads FLATTENED review data to BigQuery.\n",
    "    Each review is stored as a separate row.\n",
    "    \"\"\"\n",
    "    table_id = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
    "    \n",
    "    try:\n",
    "        # Flatten reviews into individual rows\n",
    "        df = flatten_reviews_to_rows(review_data)\n",
    "        \n",
    "        if df.empty:\n",
    "            logger.warning(\"No reviews to upload\")\n",
    "            return False\n",
    "        \n",
    "        # Upload to BigQuery\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            write_disposition=\"WRITE_APPEND\",\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Uploading {len(df)} review row(s)...\")\n",
    "        job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "        job.result()\n",
    "        \n",
    "        logger.info(f\"‚úÖ Uploaded {len(df)} review row(s) for place {review_data['place_id']}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error uploading: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"‚úÖ All functions defined successfully!\")\n",
    "print(\"\\nüìã Features:\")\n",
    "print(\"  üóÇÔ∏è FLATTENED schema (each review = one row)\")\n",
    "print(\"  üìä Individual columns for all review fields\")\n",
    "print(\"  ‚úÖ Reads from 'cid' column in Map_location\")\n",
    "print(\"  üîÑ Incremental processing (only new places)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 5: Check Current Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current status\n",
    "client = get_bigquery_client()\n",
    "\n",
    "if client:\n",
    "    print(\"üìä Checking current status...\\n\")\n",
    "    \n",
    "    # Check source table\n",
    "    source_table = f\"{PROJECT_ID}.{DATASET_ID}.{SOURCE_TABLE}\"\n",
    "    try:\n",
    "        table = client.get_table(source_table)\n",
    "        print(f\"‚úÖ Source table exists: {SOURCE_TABLE}\")\n",
    "        print(f\"   Total rows: {table.num_rows:,}\")\n",
    "        \n",
    "        query = f\"SELECT COUNT(DISTINCT cid) as count FROM `{source_table}` WHERE cid IS NOT NULL\"\n",
    "        result = client.query(query).to_dataframe()\n",
    "        print(f\"   Places with CID: {result['count'].iloc[0]:,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Source table error: {e}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Check destination table\n",
    "    dest_table = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
    "    try:\n",
    "        table = client.get_table(dest_table)\n",
    "        print(f\"‚úÖ Destination table exists: {DESTINATION_TABLE}\")\n",
    "        print(f\"   Schema: FLATTENED (each review = one row)\")\n",
    "        print(f\"   Total review rows: {table.num_rows:,}\")\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(DISTINCT place_id) as places,\n",
    "            COUNT(*) as total_reviews,\n",
    "            AVG(rating) as avg_rating,\n",
    "            MAX(timestamp) as last_fetch\n",
    "        FROM `{dest_table}`\n",
    "        \"\"\"\n",
    "        result = client.query(query).to_dataframe()\n",
    "        print(f\"   Places processed: {result['places'].iloc[0]:,}\")\n",
    "        print(f\"   Total reviews: {result['total_reviews'].iloc[0]:,}\")\n",
    "        print(f\"   Avg rating: {result['avg_rating'].iloc[0]:.2f} ‚≠ê\")\n",
    "        print(f\"   Last fetch: {result['last_fetch'].iloc[0]}\")\n",
    "        \n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è Destination table doesn't exist (will be created)\")\n",
    "        print(f\"   Will use FLATTENED schema: each review = one row\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"‚ùå Failed to connect to BigQuery\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 6: Fetch Reviews - Single Place (Test)\n",
    "\n",
    "Test fetching and flattening reviews for a single place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a single place ID\n",
    "test_place_id = \"7632417579134624850\"  # Your test CID\n",
    "\n",
    "print(f\"üß™ Testing with CID: {test_place_id}\\n\")\n",
    "\n",
    "# Fetch reviews\n",
    "review_data = fetch_all_reviews_for_place(test_place_id)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä API RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Place ID: {review_data['place_id']}\")\n",
    "print(f\"Total Reviews: {review_data['total_reviews']}\")\n",
    "print(f\"Pages Fetched: {review_data['pages_fetched']}\")\n",
    "print(f\"Topics: {len(review_data['topics'])}\")\n",
    "\n",
    "# Flatten to DataFrame\n",
    "print(\"\\nüìã Flattening reviews to table format...\")\n",
    "df_flattened = flatten_reviews_to_rows(review_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üóÇÔ∏è FLATTENED DATA PREVIEW\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total rows (one per review): {len(df_flattened)}\")\n",
    "print(f\"\\nColumns: {list(df_flattened.columns)}\")\n",
    "\n",
    "print(\"\\nüìä First 5 reviews (flattened):\")\n",
    "display(df_flattened[['rating', 'date', 'reviewer_name', 'snippet']].head())\n",
    "\n",
    "print(\"\\n‚úÖ Data successfully flattened!\")\n",
    "print(\"Each review is now a separate row with individual columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì§ Step 7: Upload Test Data to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload flattened test data\n",
    "client = get_bigquery_client()\n",
    "\n",
    "if client and 'review_data' in locals():\n",
    "    print(\"üì§ Uploading FLATTENED test data to BigQuery...\\n\")\n",
    "    \n",
    "    if create_reviews_table_if_not_exists(client):\n",
    "        if upload_review_data_to_bigquery(client, review_data):\n",
    "            print(\"\\n‚úÖ Test data uploaded successfully!\")\n",
    "            print(f\"üìä Table: {PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\")\n",
    "            print(f\"üóÇÔ∏è Format: {review_data['total_reviews']} reviews = {review_data['total_reviews']} rows\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå Failed to upload\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Failed to create table\")\n",
    "else:\n",
    "    print(\"‚ùå No data or client unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Step 8: Batch Process All Places\n",
    "\n",
    "Process all places and store reviews in **FLATTENED** format (each review = one row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch process all places\n",
    "client = get_bigquery_client()\n",
    "\n",
    "if not client:\n",
    "    print(\"‚ùå Failed to connect to BigQuery\")\n",
    "else:\n",
    "    print(\"üöÄ Starting batch processing...\\n\")\n",
    "    \n",
    "    if not create_reviews_table_if_not_exists(client):\n",
    "        print(\"‚ùå Failed to create table\")\n",
    "    else:\n",
    "        # Get CIDs to process\n",
    "        place_ids = get_place_ids_to_process(client, limit=5)  # Remove limit for full run\n",
    "        \n",
    "        if not place_ids:\n",
    "            print(\"‚úÖ No new places to process!\")\n",
    "        else:\n",
    "            print(f\"üìä Processing {len(place_ids)} place(s)...\\n\")\n",
    "            \n",
    "            successful = 0\n",
    "            failed = 0\n",
    "            skipped = 0\n",
    "            total_review_rows = 0\n",
    "            \n",
    "            for idx, place_id in enumerate(place_ids, 1):\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(f\"üìç Place {idx}/{len(place_ids)}: {place_id}\")\n",
    "                print(\"=\"*60)\n",
    "                \n",
    "                try:\n",
    "                    review_data = fetch_all_reviews_for_place(place_id)\n",
    "                    \n",
    "                    if review_data['total_reviews'] == 0:\n",
    "                        print(f\"‚ö†Ô∏è No reviews found, skipping\")\n",
    "                        skipped += 1\n",
    "                        continue\n",
    "                    \n",
    "                    if upload_review_data_to_bigquery(client, review_data):\n",
    "                        successful += 1\n",
    "                        total_review_rows += review_data['total_reviews']\n",
    "                        print(f\"‚úÖ Success: {review_data['total_reviews']} review rows uploaded\")\n",
    "                        print(f\"   üìä {len(review_data['topics'])} topics found\")\n",
    "                    else:\n",
    "                        failed += 1\n",
    "                        print(f\"‚ùå Upload failed\")\n",
    "                        \n",
    "                except KeyboardInterrupt:\n",
    "                    print(f\"\\n‚ö†Ô∏è Interrupted! Progress: {successful} done, {failed} failed\")\n",
    "                    break\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    failed += 1\n",
    "                    print(f\"‚ùå Error: {e}\")\n",
    "                \n",
    "                if idx < len(place_ids):\n",
    "                    time.sleep(1)\n",
    "            \n",
    "            # Summary\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"üìä SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"‚úÖ Successful: {successful} places\")\n",
    "            print(f\"‚ùå Failed: {failed} places\")\n",
    "            print(f\"‚è≠Ô∏è Skipped: {skipped} places\")\n",
    "            print(f\"üìä Total Review Rows: {total_review_rows:,}\")\n",
    "            if successful > 0:\n",
    "                print(f\"üìä Avg Reviews/Place: {total_review_rows/successful:.1f}\")\n",
    "            print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 9: Query Flattened Review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the flattened review data\n",
    "client = get_bigquery_client()\n",
    "\n",
    "if client:\n",
    "    table_name = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
    "    \n",
    "    try:\n",
    "        print(\"üìä Flattened Review Data Statistics\\n\")\n",
    "        \n",
    "        stats_query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(DISTINCT place_id) as total_places,\n",
    "            COUNT(*) as total_review_rows,\n",
    "            AVG(rating) as avg_rating,\n",
    "            COUNT(DISTINCT reviewer_name) as unique_reviewers,\n",
    "            MAX(timestamp) as last_fetch\n",
    "        FROM `{table_name}`\n",
    "        \"\"\"\n",
    "        \n",
    "        stats = client.query(stats_query).to_dataframe()\n",
    "        display(stats)\n",
    "        \n",
    "        print(\"\\nüìà Sample Review Rows:\")\n",
    "        sample_query = f\"\"\"\n",
    "        SELECT \n",
    "            place_id,\n",
    "            rating,\n",
    "            date,\n",
    "            reviewer_name,\n",
    "            LEFT(snippet, 100) as snippet_preview\n",
    "        FROM `{table_name}`\n",
    "        ORDER BY timestamp DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        \n",
    "        samples = client.query(sample_query).to_dataframe()\n",
    "        display(samples)\n",
    "        \n",
    "        print(\"\\n‚úÖ Query completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå No client\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Step 10: Analyze Review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze flattened review data\n",
    "client = get_bigquery_client()\n",
    "\n",
    "if client:\n",
    "    table_name = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
    "    \n",
    "    print(\"üìä Review Analysis\\n\")\n",
    "    \n",
    "    # Rating distribution\n",
    "    print(\"‚≠ê Rating Distribution:\")\n",
    "    rating_query = f\"\"\"\n",
    "    SELECT \n",
    "        rating,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "    FROM `{table_name}`\n",
    "    WHERE rating IS NOT NULL\n",
    "    GROUP BY rating\n",
    "    ORDER BY rating DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        ratings = client.query(rating_query).to_dataframe()\n",
    "        display(ratings)\n",
    "        \n",
    "        # Top reviewers\n",
    "        print(\"\\nüë• Top Reviewers:\")\n",
    "        reviewers_query = f\"\"\"\n",
    "        SELECT \n",
    "            reviewer_name,\n",
    "            COUNT(*) as reviews_in_dataset,\n",
    "            AVG(rating) as avg_rating,\n",
    "            MAX(reviewer_reviews) as total_google_reviews\n",
    "        FROM `{table_name}`\n",
    "        WHERE reviewer_name IS NOT NULL\n",
    "        GROUP BY reviewer_name\n",
    "        ORDER BY reviews_in_dataset DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewers = client.query(reviewers_query).to_dataframe()\n",
    "        display(reviewers)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå No client\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Documentation\n",
    "\n",
    "### Table Schema (FLATTENED):\n",
    "\n",
    "**Table:** `place_reviews_full`\n",
    "\n",
    "**Structure:** Each review = One row\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `place_id` | STRING | Place CID from Map_location |\n",
    "| `rating` | INTEGER | Review rating (1-5) |\n",
    "| `date` | STRING | Relative date (e.g., \"2 months ago\") |\n",
    "| `isoDate` | TIMESTAMP | ISO 8601 timestamp |\n",
    "| `snippet` | STRING | Full review text |\n",
    "| `likes` | INTEGER | Number of likes |\n",
    "| `reviewer_name` | STRING | Reviewer name |\n",
    "| `reviewer_link` | STRING | Reviewer profile link |\n",
    "| `reviewer_thumbnail` | STRING | Reviewer profile image |\n",
    "| `reviewer_reviews` | INTEGER | Reviewer's total reviews |\n",
    "| `reviewer_photos` | INTEGER | Reviewer's total photos |\n",
    "| `timestamp` | TIMESTAMP | When inserted to BigQuery |\n",
    "| `fetch_date` | DATE | Date of fetch |\n",
    "\n",
    "### Example Queries:\n",
    "\n",
    "**Get all reviews for a place:**\n",
    "```sql\n",
    "SELECT *\n",
    "FROM `shopper-reviews-477306.place_data.place_reviews_full`\n",
    "WHERE place_id = '7632417579134624850'\n",
    "ORDER BY isoDate DESC\n",
    "```\n",
    "\n",
    "**Get average rating per place:**\n",
    "```sql\n",
    "SELECT \n",
    "    place_id,\n",
    "    COUNT(*) as review_count,\n",
    "    AVG(rating) as avg_rating\n",
    "FROM `shopper-reviews-477306.place_data.place_reviews_full`\n",
    "GROUP BY place_id\n",
    "ORDER BY review_count DESC\n",
    "```\n",
    "\n",
    "**Find reviews by rating:**\n",
    "```sql\n",
    "SELECT place_id, reviewer_name, rating, snippet\n",
    "FROM `shopper-reviews-477306.place_data.place_reviews_full`\n",
    "WHERE rating = 5\n",
    "LIMIT 100\n",
    "```\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "1. **Reads CIDs** from `Map_location.cid` column\n",
    "2. **Fetches reviews** via Google Reviews API (with pagination)\n",
    "3. **Flattens data**: Each review becomes one row\n",
    "4. **Uploads to BigQuery**: Structured table format\n",
    "5. **Incremental**: Only processes new places\n",
    "\n",
    "### Key Benefits:\n",
    "\n",
    "‚úÖ **No JSON parsing** needed - direct column access\n",
    "‚úÖ **Easy queries** - standard SQL on individual columns\n",
    "‚úÖ **Better performance** - indexed columns, faster queries\n",
    "‚úÖ **Clear structure** - one review per row\n",
    "\n",
    "---\n",
    "\n",
    "**Created for Google Colab** | **Last updated: 2025-11-05** | **Version: 2.0 - Flattened Schema**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Review_Fetcher_Colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
