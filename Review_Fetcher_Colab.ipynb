{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåü Google Reviews Data Fetcher - Google Colab\n",
    "\n",
    "This notebook fetches complete review data from Google Reviews API (via RapidAPI) and stores it in BigQuery.\n",
    "\n",
    "## Features:\n",
    "- üìä Fetches ALL review data (reviews, topics, metadata)\n",
    "- üîÑ Automatic pagination (follows nextPageToken)\n",
    "- üíæ Stores complete data in BigQuery\n",
    "- ‚ö° Incremental processing (only new places)\n",
    "- üõ°Ô∏è Robust error handling and retries\n",
    "- üìà Progress tracking and logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-cloud-bigquery google-auth pandas db-dtypes\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import http.client\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "from typing import Optional, Dict, Any, List\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "from google.colab import userdata\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Step 3: Configure API Credentials\n",
    "\n",
    "### Option A: Using Colab Secrets (Recommended)\n",
    "1. Click on the üîë key icon in the left sidebar\n",
    "2. Add a secret named `RAPIDAPI_KEY` with your API key\n",
    "3. Add a secret named `BIGQUERY_KEY_JSON` with your service account JSON\n",
    "\n",
    "### Option B: Manual Configuration\n",
    "Uncomment and fill in the credentials below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get credentials from Colab secrets first\n",
    "try:\n",
    "    RAPIDAPI_KEY = userdata.get('RAPIDAPI_KEY')\n",
    "    print(\"‚úÖ RapidAPI key loaded from Colab secrets\")\n",
    "except:\n",
    "    # Manual configuration - uncomment and fill in\n",
    "    RAPIDAPI_KEY = \"ac0025f410mshd0c260cb60f3db6p18c4b0jsnc9b7413cd574\"  # Your API key\n",
    "    print(\"‚ö†Ô∏è RapidAPI key loaded from manual configuration\")\n",
    "\n",
    "# Load BigQuery credentials from secrets\n",
    "try:\n",
    "    BIGQUERY_CREDENTIALS_STR = userdata.get('BIGQUERY_KEY_JSON')\n",
    "    BIGQUERY_CREDENTIALS = json.loads(BIGQUERY_CREDENTIALS_STR)\n",
    "    print(\"‚úÖ BigQuery credentials loaded from Colab secrets\")\n",
    "    PROJECT_ID = BIGQUERY_CREDENTIALS.get('project_id', 'shopper-reviews-477306')\n",
    "except:\n",
    "    # Fallback to manual configuration\n",
    "    print(\"‚ö†Ô∏è BigQuery credentials loaded from manual configuration\")\n",
    "    PROJECT_ID = \"shopper-reviews-477306\"\n",
    "    BIGQUERY_CREDENTIALS = {\n",
    "        \"type\": \"service_account\",\n",
    "        \"project_id\": \"shopper-reviews-477306\",\n",
    "        \"private_key_id\": \"679b00310997262ff77901f080075b509eb9c770\",\n",
    "        \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCPrVXmepJWx8A8\\nXLqDARbLqqmgPwQ4NEmCCOmAZ019aFToc0Yho0/hDyMhRhsW6z/5h8YVEbheb2oR\\nmFK6/v3UEN1Mf6oJWag9pBngM6IO96QAzozjXjCmIVYJku1HWi+7b4mX7La8p77N\\n5fJdOh30ceC6cJSDA51r2xGJDmchRPNhRR8CS9u3xAeZZeB/pgShwJcLM4WY4L3P\\niwc7qkQb91NPbB2/p3hL/JJAtCvVKf61xlWGOKEGW3pIwBUUcF2/OJ3FTuWrY7P8\\n1c/Kz9LUYOZpztK9zjFCNcnCQvvVAow9bqg3fw6xqE172dQT1FG6AieFSCyUib5B\\nXxwNu0phAgMBAAECggEAET1ThPqIxqA54RmgnjQqP7k0Q0XBxDCvRUq7zIFuBdyC\\nm6Wr8OtUnAT3Snh2qv2tSSFRKO6zDaRsDhJrPYQigX3zNR5Nu8jQlseIUfjqusWy\\nHbqq+GPb4y3gJ06Zk/8uolyUHkZJTZe0cvuNZOxNSIBwM6QV3dE4OVx+3SV88GZ/\\nOkAMCUpPRLJux6vJo+l0Qcfe074qjRYPv3XUaGXyHXeOZXmze/lLF6wsEzZmP1A+\\nE9xZmP4ucM3ybrYi3ipRu6YwuR2mRASLy8VFMtcYCvNZGv6ODkjF2xmpucHwX78S\\nzO3mGFES3Hnknjzoif5sJuBewNSztXJcQqKgtSpDhQKBgQDCS6bYj1VR691J5wxA\\n5/fl2MwY4ALIKqW4RtJyNRBZ7+WDAVkq99R6lz+AmQsb6QyiZ/yTZHSUI61Bjn0p\\nd2MD/fpQle7ZOMyR1gKZk5fE5lvmfA5sK+Aax3dRI7xjPBXJYI4hiCMAxgYdhgtI\\nG1C/Nf6O2HoE/W2qLEnLZadpowKBgQC9Tl+/9Eq9Q/DI74CG78U0+s2aRq19vsXZ\\n+wCIUm54TcN9xw4nPKYbT24nTVwTrOu2bxEgDVmuAqtWlKGad16LqZFTZ2aUaEFC\\ni1HL8UKSy5XmNcum8mrKL5+MvwExcQUSmalE3PEQDRjV65QNld0EbQ6JNz74025z\\nm+3ISpIEKwKBgADf5E1fP8wRmrplbtmv8Z64PhryjzCleH9+2h2nfX5aJRdU3zjh\\nSrSOj7uddL5YazUj8LAdKKUuD+6WnJueLPTspL7OHfgeWFVjuDlGv80kGE/OSSZV\\ngDm+ohvcZFGyCIsSgzFFcprjSU3Ct7RIYzGpJY8xDEOPfHninyZqO7mvAoGAIsog\\ndppikd3Ghmbda+7sgwwEdPHAOHeyzJiARI1BmAJShu7p/vP6YtJ6H+broQIKX4CR\\n2R4a+QusiUDPYh/F1EzZVEaQZ32xYJVR9vTjky6u4ZvJTWkHjxipbag8g+WNVRnA\\nLdOcyaJeihG9J7H+6C1Smoz4manhhoWFcWWi5/kCgYEAssgWnlZCygCjEQ/XDVtZ\\nC8/uelJnMHO93U4yF6Xk61gazKYpXpKjNkD3xfxAyQ3zkBkWo7CXg1env8pT9ld1\\nraWCeCmH/w8i0ww3Cmplks5mXIYPrPPuUCEW5D6B8hIyNC1VIoaOlva8+FgJYPIv\\nC5AqN3hBRDOUbophIQmAe5I=\\n-----END PRIVATE KEY-----\\n\",\n",
    "        \"client_email\": \"demand@shopper-reviews-477306.iam.gserviceaccount.com\",\n",
    "        \"client_id\": \"100956109416744224832\",\n",
    "        \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "        \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "        \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "        \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/demand%40shopper-reviews-477306.iam.gserviceaccount.com\",\n",
    "        \"universe_domain\": \"googleapis.com\"\n",
    "    }\n",
    "\n",
    "# BigQuery Configuration\n",
    "DATASET_ID = \"place_data\"\n",
    "SOURCE_TABLE = \"Map_location\"  # Table with place_ids\n",
    "DESTINATION_TABLE = \"place_reviews_full\"  # Table to store reviews\n",
    "\n",
    "# API Configuration\n",
    "API_HOST = \"google-search-master-mega.p.rapidapi.com\"\n",
    "MAX_PAGES = 10  # Maximum pages to fetch per place\n",
    "RETRY_ATTEMPTS = 3\n",
    "RETRY_DELAY = 2  # seconds\n",
    "\n",
    "print(\"\\n‚úÖ All configuration loaded!\")\n",
    "print(f\"üìä Source Table: {PROJECT_ID}.{DATASET_ID}.{SOURCE_TABLE}\")\n",
    "print(f\"üìä Destination Table: {PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 4: Define Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== BIGQUERY CLIENT ====================\n",
    "\n",
    "def get_bigquery_client() -> Optional[bigquery.Client]:\n",
    "    \"\"\"\n",
    "    Creates and returns a BigQuery client with proper credentials.\n",
    "    \n",
    "    Returns:\n",
    "        BigQuery client or None on error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        credentials = service_account.Credentials.from_service_account_info(\n",
    "            BIGQUERY_CREDENTIALS,\n",
    "            scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    "        )\n",
    "        client = bigquery.Client(credentials=credentials, project=PROJECT_ID)\n",
    "        logger.info(f\"‚úÖ Connected to BigQuery project: {PROJECT_ID}\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error creating BigQuery client: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ==================== API FUNCTIONS ====================\n",
    "\n",
    "def fetch_reviews_for_place(place_id: str, page: int = 1) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetches review data for a single place from Google Reviews API.\n",
    "    \n",
    "    Args:\n",
    "        place_id: The place CID to fetch reviews for\n",
    "        page: Page number to fetch (for pagination)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing full API response or None on error\n",
    "    \"\"\"\n",
    "    for attempt in range(RETRY_ATTEMPTS):\n",
    "        try:\n",
    "            conn = http.client.HTTPSConnection(API_HOST)\n",
    "            \n",
    "            headers = {\n",
    "                'x-rapidapi-key': RAPIDAPI_KEY,\n",
    "                'x-rapidapi-host': API_HOST\n",
    "            }\n",
    "            \n",
    "            # Build query parameters\n",
    "            params = f\"?cid={place_id}&sortBy=mostRelevant&gl=us&hl=en&page={page}\"\n",
    "            endpoint = \"/reviews\" + params\n",
    "            \n",
    "            logger.info(f\"üì° Fetching page {page} for place {place_id}...\")\n",
    "            \n",
    "            conn.request(\"GET\", endpoint, headers=headers)\n",
    "            res = conn.getresponse()\n",
    "            data = res.read()\n",
    "            \n",
    "            if res.status == 200:\n",
    "                result = json.loads(data.decode(\"utf-8\"))\n",
    "                logger.info(f\"‚úÖ Successfully fetched page {page} for place {place_id}\")\n",
    "                return result\n",
    "            else:\n",
    "                logger.warning(f\"‚ö†Ô∏è API returned status {res.status}, attempt {attempt + 1}/{RETRY_ATTEMPTS}\")\n",
    "                if attempt < RETRY_ATTEMPTS - 1:\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error fetching reviews, attempt {attempt + 1}/{RETRY_ATTEMPTS}: {e}\")\n",
    "            if attempt < RETRY_ATTEMPTS - 1:\n",
    "                time.sleep(RETRY_DELAY)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def fetch_all_reviews_for_place(place_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetches ALL reviews for a place by following pagination.\n",
    "    \n",
    "    Args:\n",
    "        place_id: The place CID to fetch reviews for\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing aggregated review data\n",
    "    \"\"\"\n",
    "    all_reviews = []\n",
    "    all_topics = []\n",
    "    metadata = {}\n",
    "    page = 1\n",
    "    \n",
    "    logger.info(f\"üîç Starting to fetch all reviews for place {place_id}...\")\n",
    "    \n",
    "    while page <= MAX_PAGES:\n",
    "        result = fetch_reviews_for_place(place_id, page)\n",
    "        \n",
    "        if not result:\n",
    "            logger.warning(f\"‚ö†Ô∏è No data received for page {page}, stopping pagination\")\n",
    "            break\n",
    "        \n",
    "        # Extract reviews from this page\n",
    "        reviews = result.get('reviews', [])\n",
    "        all_reviews.extend(reviews)\n",
    "        \n",
    "        # Extract topics (usually same across pages, take from first page)\n",
    "        if page == 1:\n",
    "            all_topics = result.get('topics', [])\n",
    "            metadata = {\n",
    "                'searchParameters': result.get('searchParameters', {}),\n",
    "                'credits': result.get('credits', 0),\n",
    "            }\n",
    "        \n",
    "        logger.info(f\"‚úÖ Page {page}: {len(reviews)} reviews fetched\")\n",
    "        \n",
    "        # Check for next page\n",
    "        next_page_token = result.get('nextPageToken')\n",
    "        if not next_page_token or len(reviews) == 0:\n",
    "            logger.info(f\"‚úÖ No more pages available, stopping at page {page}\")\n",
    "            break\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(0.5)  # Rate limiting\n",
    "    \n",
    "    logger.info(f\"üéâ Completed fetching for place {place_id}: {len(all_reviews)} total reviews, {len(all_topics)} topics\")\n",
    "    \n",
    "    return {\n",
    "        'place_id': place_id,\n",
    "        'total_reviews': len(all_reviews),\n",
    "        'reviews': all_reviews,\n",
    "        'topics': all_topics,\n",
    "        'metadata': metadata,\n",
    "        'pages_fetched': page,\n",
    "        'timestamp': datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "\n",
    "\n",
    "# ==================== BIGQUERY OPERATIONS ====================\n",
    "\n",
    "def get_place_ids_to_process(client: bigquery.Client, limit: int = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieves place IDs from the source table that need review data fetched.\n",
    "    \n",
    "    Args:\n",
    "        client: BigQuery client\n",
    "        limit: Optional limit on number of places to fetch\n",
    "        \n",
    "    Returns:\n",
    "        List of place_id strings\n",
    "    \"\"\"\n",
    "    source_table = f\"{PROJECT_ID}.{DATASET_ID}.{SOURCE_TABLE}\"\n",
    "    \n",
    "    try:\n",
    "        dest_table = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
    "        \n",
    "        try:\n",
    "            client.get_table(dest_table)\n",
    "            # Table exists, exclude already processed places\n",
    "            query = f\"\"\"\n",
    "            SELECT DISTINCT cid as place_id\n",
    "            FROM `{source_table}`\n",
    "            WHERE cid IS NOT NULL\n",
    "            AND cid NOT IN (\n",
    "                SELECT DISTINCT place_id\n",
    "                FROM `{dest_table}`\n",
    "                WHERE place_id IS NOT NULL\n",
    "            )\n",
    "            \"\"\"\n",
    "            if limit:\n",
    "                query += f\" LIMIT {limit}\"\n",
    "            logger.info(\"üìä Fetching place_ids that haven't been processed yet...\")\n",
    "        except:\n",
    "            # Table doesn't exist yet, process all\n",
    "            query = f\"\"\"\n",
    "            SELECT DISTINCT cid as place_id\n",
    "            FROM `{source_table}`\n",
    "            WHERE cid IS NOT NULL\n",
    "            \"\"\"\n",
    "            if limit:\n",
    "                query += f\" LIMIT {limit}\"\n",
    "            logger.info(\"üìä Destination table doesn't exist, fetching all place_ids...\")\n",
    "        \n",
    "        result = client.query(query).to_dataframe()\n",
    "        place_ids = result['place_id'].tolist()\n",
    "        \n",
    "        logger.info(f\"‚úÖ Found {len(place_ids)} place(s) to process\")\n",
    "        return place_ids\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error fetching place IDs: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def create_reviews_table_if_not_exists(client: bigquery.Client) -> bool:\n",
    "    \"\"\"\n",
    "    Creates the place_reviews_full table if it doesn't exist.\n",
    "    \n",
    "    Args:\n",
    "        client: BigQuery client\n",
    "        \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    table_id = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
    "    \n",
    "    try:\n",
    "        # Check if table exists\n",
    "        try:\n",
    "            client.get_table(table_id)\n",
    "            logger.info(f\"‚úÖ Table {DESTINATION_TABLE} already exists\")\n",
    "            return True\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Create table with schema\n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"place_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField(\"total_reviews\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"pages_fetched\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"reviews\", \"STRING\"),  # JSON stored as STRING\n",
    "            bigquery.SchemaField(\"topics\", \"STRING\"),   # JSON stored as STRING\n",
    "            bigquery.SchemaField(\"metadata\", \"STRING\"), # JSON stored as STRING\n",
    "            bigquery.SchemaField(\"timestamp\", \"TIMESTAMP\"),\n",
    "            bigquery.SchemaField(\"fetch_date\", \"DATE\"),\n",
    "        ]\n",
    "        \n",
    "        table = bigquery.Table(table_id, schema=schema)\n",
    "        table = client.create_table(table)\n",
    "        \n",
    "        logger.info(f\"‚úÖ Created table {DESTINATION_TABLE}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error creating table: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def upload_review_data_to_bigquery(client: bigquery.Client, review_data: Dict[str, Any]) -> bool:\n",
    "    \"\"\"\n",
    "    Uploads review data to BigQuery.\n",
    "    \n",
    "    Args:\n",
    "        client: BigQuery client\n",
    "        review_data: Dictionary containing review data\n",
    "        \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    table_id = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
    "    \n",
    "    try:\n",
    "        # Prepare data for upload\n",
    "        row = {\n",
    "            'place_id': review_data['place_id'],\n",
    "            'total_reviews': review_data['total_reviews'],\n",
    "            'pages_fetched': review_data['pages_fetched'],\n",
    "            'reviews': json.dumps(review_data['reviews']),\n",
    "            'topics': json.dumps(review_data['topics']),\n",
    "            'metadata': json.dumps(review_data['metadata']),\n",
    "            'timestamp': datetime.now(timezone.utc),\n",
    "            'fetch_date': datetime.now(timezone.utc).date(),\n",
    "        }\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame([row])\n",
    "        \n",
    "        # Upload to BigQuery\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            write_disposition=\"WRITE_APPEND\",\n",
    "        )\n",
    "        \n",
    "        job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "        job.result()  # Wait for job to complete\n",
    "        \n",
    "        logger.info(f\"‚úÖ Uploaded review data for place {review_data['place_id']} to BigQuery\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error uploading to BigQuery: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"‚úÖ All functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 5: Check Current Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current status of tables and data\n",
    "client = get_bigquery_client()\n",
    "\n",
    "if client:\n",
    "    print(\"üìä Checking current status...\\n\")\n",
    "    \n",
    "    # Check source table\n",
    "    source_table = f\"{PROJECT_ID}.{DATASET_ID}.{SOURCE_TABLE}\"\n",
    "    try:\n",
    "        table = client.get_table(source_table)\n",
    "        print(f\"‚úÖ Source table exists: {SOURCE_TABLE}\")\n",
    "        print(f\"   Total rows: {table.num_rows:,}\")\n",
    "        \n",
    "        # Count places with cid\n",
    "        query = f\"SELECT COUNT(DISTINCT cid) as count FROM `{source_table}` WHERE cid IS NOT NULL\"\n",
    "        result = client.query(query).to_dataframe()\n",
    "        print(f\"   Places with CID: {result['count'].iloc[0]:,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Source table not found: {e}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Check destination table\n",
    "    dest_table = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
    "    try:\n",
    "        table = client.get_table(dest_table)\n",
    "        print(f\"‚úÖ Destination table exists: {DESTINATION_TABLE}\")\n",
    "        print(f\"   Total rows: {table.num_rows:,}\")\n",
    "        \n",
    "        # Get summary stats\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(DISTINCT place_id) as places,\n",
    "            SUM(total_reviews) as total_reviews,\n",
    "            AVG(total_reviews) as avg_reviews,\n",
    "            MAX(timestamp) as last_fetch\n",
    "        FROM `{dest_table}`\n",
    "        \"\"\"\n",
    "        result = client.query(query).to_dataframe()\n",
    "        print(f\"   Places processed: {result['places'].iloc[0]:,}\")\n",
    "        print(f\"   Total reviews: {result['total_reviews'].iloc[0]:,}\")\n",
    "        print(f\"   Avg reviews/place: {result['avg_reviews'].iloc[0]:.1f}\")\n",
    "        print(f\"   Last fetch: {result['last_fetch'].iloc[0]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Destination table doesn't exist yet (will be created)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"‚ùå Failed to connect to BigQuery\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 6: Fetch Reviews - Single Place (Test)\n",
    "\n",
    "Test fetching reviews for a single place before processing in batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a single place ID\n",
    "test_place_id = \"17602107806865671526\"  # Example place ID\n",
    "\n",
    "print(f\"üß™ Testing with place ID: {test_place_id}\\n\")\n",
    "\n",
    "# Fetch reviews\n",
    "review_data = fetch_all_reviews_for_place(test_place_id)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Place ID: {review_data['place_id']}\")\n",
    "print(f\"Total Reviews: {review_data['total_reviews']}\")\n",
    "print(f\"Pages Fetched: {review_data['pages_fetched']}\")\n",
    "print(f\"Topics Found: {len(review_data['topics'])}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show sample review\n",
    "if review_data['reviews']:\n",
    "    print(\"\\nüìù Sample Review:\")\n",
    "    sample = review_data['reviews'][0]\n",
    "    print(f\"Rating: {sample.get('rating', 'N/A')} ‚≠ê\")\n",
    "    print(f\"Date: {sample.get('date', 'N/A')}\")\n",
    "    print(f\"Snippet: {sample.get('snippet', 'N/A')[:200]}...\")\n",
    "    print(f\"\\nReviewer: {sample.get('user', {}).get('name', 'N/A')}\")\n",
    "    print(f\"Reviewer Reviews: {sample.get('user', {}).get('reviews', 'N/A')}\")\n",
    "\n",
    "# Show topics\n",
    "if review_data['topics']:\n",
    "    print(\"\\nüìã Topics:\")\n",
    "    for topic in review_data['topics'][:5]:  # Show first 5\n",
    "        print(f\"  - {topic.get('name', 'N/A')}: {topic.get('reviews', 0)} mentions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì§ Step 7: Upload Test Data to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the test data to BigQuery\n",
    "client = get_bigquery_client()\n",
    "\n",
    "if client and 'review_data' in locals():\n",
    "    print(\"üì§ Uploading test data to BigQuery...\\n\")\n",
    "    \n",
    "    # Create table if needed\n",
    "    if create_reviews_table_if_not_exists(client):\n",
    "        # Upload data\n",
    "        if upload_review_data_to_bigquery(client, review_data):\n",
    "            print(\"\\n‚úÖ Test data uploaded successfully!\")\n",
    "            print(f\"üìä Table: {PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå Failed to upload test data\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Failed to create table\")\n",
    "else:\n",
    "    print(\"‚ùå No test data to upload or BigQuery client not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Step 8: Batch Process All Places\n",
    "\n",
    "Process all places from the source table that haven't been fetched yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch process all places\n",
    "client = get_bigquery_client()\n",
    "\n",
    "if not client:\n",
    "    print(\"‚ùå Failed to connect to BigQuery\")\n",
    "else:\n",
    "    print(\"üöÄ Starting batch processing...\\n\")\n",
    "    \n",
    "    # Create destination table if needed\n",
    "    if not create_reviews_table_if_not_exists(client):\n",
    "        print(\"‚ùå Failed to create destination table\")\n",
    "    else:\n",
    "        # Get place IDs to process (limit to 5 for this demo, remove limit for full run)\n",
    "        place_ids = get_place_ids_to_process(client, limit=5)  # Remove limit=5 for full processing\n",
    "        \n",
    "        if not place_ids:\n",
    "            print(\"‚úÖ No new places to process!\")\n",
    "        else:\n",
    "            print(f\"üìä Processing {len(place_ids)} place(s)...\\n\")\n",
    "            \n",
    "            # Track results\n",
    "            successful = 0\n",
    "            failed = 0\n",
    "            total_reviews = 0\n",
    "            \n",
    "            # Process each place\n",
    "            for idx, place_id in enumerate(place_ids, 1):\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(f\"üìç Processing place {idx}/{len(place_ids)}: {place_id}\")\n",
    "                print(\"=\"*60)\n",
    "                \n",
    "                try:\n",
    "                    # Fetch all review data\n",
    "                    review_data = fetch_all_reviews_for_place(place_id)\n",
    "                    \n",
    "                    if review_data['total_reviews'] == 0:\n",
    "                        print(f\"‚ö†Ô∏è No reviews found for place {place_id}, skipping\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Upload to BigQuery\n",
    "                    if upload_review_data_to_bigquery(client, review_data):\n",
    "                        successful += 1\n",
    "                        total_reviews += review_data['total_reviews']\n",
    "                        print(f\"‚úÖ Successfully processed place {place_id}\")\n",
    "                        print(f\"   üìä {review_data['total_reviews']} reviews, {len(review_data['topics'])} topics\")\n",
    "                    else:\n",
    "                        failed += 1\n",
    "                        print(f\"‚ùå Failed to upload data for place {place_id}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    failed += 1\n",
    "                    print(f\"‚ùå Error processing place {place_id}: {e}\")\n",
    "                \n",
    "                # Rate limiting between places\n",
    "                if idx < len(place_ids):\n",
    "                    time.sleep(1)\n",
    "            \n",
    "            # Print summary\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"üìä PROCESSING SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"‚úÖ Successful: {successful}\")\n",
    "            print(f\"‚ùå Failed: {failed}\")\n",
    "            print(f\"üìä Total Reviews Fetched: {total_reviews:,}\")\n",
    "            print(f\"üìä Avg Reviews/Place: {total_reviews/successful if successful > 0 else 0:.1f}\")\n",
    "            print(\"=\"*60)\n",
    "            print(\"üéâ Batch processing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 9: Query and Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query and analyze the review data\n",
    "client = get_bigquery_client()\n",
    "\n",
    "if client:\n",
    "    table_name = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
    "    \n",
    "    try:\n",
    "        # Overall statistics\n",
    "        print(\"üìä Review Data Statistics\\n\")\n",
    "        \n",
    "        stats_query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(DISTINCT place_id) as total_places,\n",
    "            SUM(total_reviews) as total_reviews,\n",
    "            AVG(total_reviews) as avg_reviews_per_place,\n",
    "            MIN(total_reviews) as min_reviews,\n",
    "            MAX(total_reviews) as max_reviews,\n",
    "            MAX(timestamp) as last_fetch\n",
    "        FROM `{table_name}`\n",
    "        \"\"\"\n",
    "        \n",
    "        stats = client.query(stats_query).to_dataframe()\n",
    "        display(stats)\n",
    "        \n",
    "        # Places by review count\n",
    "        print(\"\\nüìà Places by Review Count:\")\n",
    "        \n",
    "        places_query = f\"\"\"\n",
    "        SELECT \n",
    "            place_id,\n",
    "            total_reviews,\n",
    "            pages_fetched,\n",
    "            timestamp\n",
    "        FROM `{table_name}`\n",
    "        ORDER BY total_reviews DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        \n",
    "        places = client.query(places_query).to_dataframe()\n",
    "        display(places)\n",
    "        \n",
    "        print(\"\\n‚úÖ Data query completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error querying data: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to connect to BigQuery\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 10: Extract and View Individual Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract individual reviews from JSON for a specific place\n",
    "client = get_bigquery_client()\n",
    "\n",
    "if client:\n",
    "    table_name = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
    "    \n",
    "    # Get a sample place\n",
    "    sample_query = f\"SELECT place_id FROM `{table_name}` LIMIT 1\"\n",
    "    sample_result = client.query(sample_query).to_dataframe()\n",
    "    \n",
    "    if not sample_result.empty:\n",
    "        sample_place_id = sample_result['place_id'].iloc[0]\n",
    "        \n",
    "        print(f\"üìù Extracting reviews for place: {sample_place_id}\\n\")\n",
    "        \n",
    "        # Extract individual reviews using JSON functions\n",
    "        reviews_query = f\"\"\"\n",
    "        SELECT \n",
    "            place_id,\n",
    "            JSON_VALUE(review, '$.rating') as rating,\n",
    "            JSON_VALUE(review, '$.date') as date,\n",
    "            JSON_VALUE(review, '$.snippet') as snippet,\n",
    "            JSON_VALUE(review, '$.user.name') as reviewer_name,\n",
    "            CAST(JSON_VALUE(review, '$.user.reviews') AS INT64) as reviewer_total_reviews,\n",
    "            CAST(JSON_VALUE(review, '$.likes') AS INT64) as likes\n",
    "        FROM `{table_name}`,\n",
    "        UNNEST(JSON_EXTRACT_ARRAY(reviews)) as review\n",
    "        WHERE place_id = '{sample_place_id}'\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews_df = client.query(reviews_query).to_dataframe()\n",
    "        display(reviews_df)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Found {len(reviews_df)} reviews\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No data in table yet\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to connect to BigQuery\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Step 11: View Topics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View topics across all places\n",
    "client = get_bigquery_client()\n",
    "\n",
    "if client:\n",
    "    table_name = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
    "    \n",
    "    print(\"üè∑Ô∏è Topic Analysis Across All Places\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Extract and aggregate topics\n",
    "        topics_query = f\"\"\"\n",
    "        SELECT \n",
    "            JSON_VALUE(topic, '$.name') as topic_name,\n",
    "            SUM(CAST(JSON_VALUE(topic, '$.reviews') AS INT64)) as total_mentions,\n",
    "            COUNT(DISTINCT place_id) as places_with_topic\n",
    "        FROM `{table_name}`,\n",
    "        UNNEST(JSON_EXTRACT_ARRAY(topics)) as topic\n",
    "        GROUP BY topic_name\n",
    "        ORDER BY total_mentions DESC\n",
    "        LIMIT 20\n",
    "        \"\"\"\n",
    "        \n",
    "        topics_df = client.query(topics_query).to_dataframe()\n",
    "        display(topics_df)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Found {len(topics_df)} unique topics\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing topics: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to connect to BigQuery\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Additional Information\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "#### **Data Collection Flow:**\n",
    "1. Reads place_ids (CID) from `Map_location` table\n",
    "2. For each place:\n",
    "   - Fetches reviews from Google Reviews API\n",
    "   - Follows pagination automatically (nextPageToken)\n",
    "   - Aggregates all reviews, topics, and metadata\n",
    "3. Stores complete data in `place_reviews_full` table\n",
    "4. Runs incrementally (only processes new places)\n",
    "\n",
    "#### **Table Schema:**\n",
    "- `place_id` (STRING): Place CID from Google Maps\n",
    "- `total_reviews` (INTEGER): Total number of reviews fetched\n",
    "- `pages_fetched` (INTEGER): Number of API pages processed\n",
    "- `reviews` (JSON): Complete array of all reviews with full data\n",
    "- `topics` (JSON): Array of topics mentioned in reviews\n",
    "- `metadata` (JSON): API metadata (searchParameters, credits, etc.)\n",
    "- `timestamp` (TIMESTAMP): When data was fetched (UTC)\n",
    "- `fetch_date` (DATE): Date of fetch\n",
    "\n",
    "#### **Review Data Structure:**\n",
    "Each review contains:\n",
    "- rating, date, isoDate, snippet, likes\n",
    "- user.name, user.link, user.thumbnail\n",
    "- user.reviews (count), user.photos (count)\n",
    "\n",
    "#### **Topic Data Structure:**\n",
    "Each topic contains:\n",
    "- name (e.g., \"studying\", \"coffee\", \"wifi\")\n",
    "- reviews (count of mentions)\n",
    "- id (Google topic identifier)\n",
    "\n",
    "### Configuration:\n",
    "\n",
    "**Rate Limiting:**\n",
    "- 0.5 seconds between pages\n",
    "- 1 second between places\n",
    "- 3 retry attempts with 2-second delays\n",
    "\n",
    "**Adjustable Parameters:**\n",
    "```python\n",
    "MAX_PAGES = 10        # Max pages per place\n",
    "RETRY_ATTEMPTS = 3    # API retry attempts\n",
    "RETRY_DELAY = 2       # Seconds between retries\n",
    "```\n",
    "\n",
    "### API Information:\n",
    "- **Provider**: RapidAPI - Google Search Master Mega\n",
    "- **Endpoint**: `/reviews`\n",
    "- **Parameters**: cid, sortBy, gl, hl, page\n",
    "\n",
    "### Tips:\n",
    "1. **Test First**: Use Step 6 to test with a single place before batch processing\n",
    "2. **Batch Limit**: Adjust `limit` parameter in Step 8 for controlled processing\n",
    "3. **Monitor Quotas**: Check your RapidAPI usage limits\n",
    "4. **Incremental**: Safe to run multiple times - only processes new places\n",
    "5. **Query Data**: Use Steps 9-11 to analyze collected data\n",
    "\n",
    "### Troubleshooting:\n",
    "- **No place IDs**: Check `Map_location` table has `cid` column\n",
    "- **API errors**: Verify RAPIDAPI_KEY is correct\n",
    "- **Upload fails**: Check BigQuery credentials and permissions\n",
    "- **Rate limits**: Increase delays between requests\n",
    "\n",
    "---\n",
    "\n",
    "**Created for Google Colab** | Last updated: 2025-11-05"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Review_Fetcher_Colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
